[2019-12-10 13:01:07,925] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:01:07,933] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:01:07,934] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:01:07,934] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:01:07,935] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-12-10 13:01:07,965] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:01:07,967] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-12-10 13:01:07,978] INFO Server environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,979] INFO Server environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,980] INFO Server environment:java.version=1.8.0_191 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,981] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,982] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,983] INFO Server environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,988] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,992] INFO Server environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,992] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,993] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,994] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,994] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,995] INFO Server environment:user.name=avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,996] INFO Server environment:user.home=C:\Users\avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:07,996] INFO Server environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:08,010] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:08,010] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:08,012] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:01:08,042] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-12-10 13:01:08,045] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:02:31,178] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:02:31,626] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:02:31,627] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:02:31,651] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:02:31,658] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,659] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,659] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,659] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,659] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,660] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,665] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,666] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,667] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,667] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,668] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,669] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,673] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,674] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,675] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,677] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:31,701] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:02:31,704] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:02:31,707] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:02:37,704] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:02:37,708] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x0 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:02:37,811] INFO Session: 0x0 closed (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:02:37,813] INFO EventThread shut down for session: 0x0 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:02:37,814] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:02:37,822] ERROR Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
	at kafka.zookeeper.ZooKeeperClient$$anonfun$kafka$zookeeper$ZooKeeperClient$$waitUntilConnected$1.apply$mcV$sp(ZooKeeperClient.scala:258)
	at kafka.zookeeper.ZooKeeperClient$$anonfun$kafka$zookeeper$ZooKeeperClient$$waitUntilConnected$1.apply(ZooKeeperClient.scala:254)
	at kafka.zookeeper.ZooKeeperClient$$anonfun$kafka$zookeeper$ZooKeeperClient$$waitUntilConnected$1.apply(ZooKeeperClient.scala:254)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.zookeeper.ZooKeeperClient.kafka$zookeeper$ZooKeeperClient$$waitUntilConnected(ZooKeeperClient.scala:254)
	at kafka.zookeeper.ZooKeeperClient.<init>(ZooKeeperClient.scala:112)
	at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1825)
	at kafka.server.KafkaServer.kafka$server$KafkaServer$$createZkClient$1(KafkaServer.scala:363)
	at kafka.server.KafkaServer.initZkClient(KafkaServer.scala:387)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:207)
	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)
	at kafka.Kafka$.main(Kafka.scala:84)
	at kafka.Kafka.main(Kafka.scala)
[2019-12-10 13:02:37,828] INFO shutting down (kafka.server.KafkaServer)
[2019-12-10 13:02:37,838] INFO shut down completed (kafka.server.KafkaServer)
[2019-12-10 13:02:37,839] ERROR Exiting Kafka. (kafka.server.KafkaServerStartable)
[2019-12-10 13:02:37,858] INFO shutting down (kafka.server.KafkaServer)
[2019-12-10 13:03:03,667] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:03:04,169] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:03:04,170] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:03:04,195] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:03:04,202] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,203] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,203] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,203] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,203] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,204] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,209] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,213] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,214] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,214] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,215] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,216] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,216] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,217] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,218] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,221] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:04,246] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:03:04,251] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:03:04,253] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:03:10,251] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:03:10,254] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x0 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:03:10,357] INFO Session: 0x0 closed (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:03:10,360] INFO EventThread shut down for session: 0x0 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:03:10,361] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:03:10,365] ERROR Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
	at kafka.zookeeper.ZooKeeperClient$$anonfun$kafka$zookeeper$ZooKeeperClient$$waitUntilConnected$1.apply$mcV$sp(ZooKeeperClient.scala:258)
	at kafka.zookeeper.ZooKeeperClient$$anonfun$kafka$zookeeper$ZooKeeperClient$$waitUntilConnected$1.apply(ZooKeeperClient.scala:254)
	at kafka.zookeeper.ZooKeeperClient$$anonfun$kafka$zookeeper$ZooKeeperClient$$waitUntilConnected$1.apply(ZooKeeperClient.scala:254)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.zookeeper.ZooKeeperClient.kafka$zookeeper$ZooKeeperClient$$waitUntilConnected(ZooKeeperClient.scala:254)
	at kafka.zookeeper.ZooKeeperClient.<init>(ZooKeeperClient.scala:112)
	at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1825)
	at kafka.server.KafkaServer.kafka$server$KafkaServer$$createZkClient$1(KafkaServer.scala:363)
	at kafka.server.KafkaServer.initZkClient(KafkaServer.scala:387)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:207)
	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)
	at kafka.Kafka$.main(Kafka.scala:84)
	at kafka.Kafka.main(Kafka.scala)
[2019-12-10 13:03:10,370] INFO shutting down (kafka.server.KafkaServer)
[2019-12-10 13:03:10,376] INFO shut down completed (kafka.server.KafkaServer)
[2019-12-10 13:03:10,377] ERROR Exiting Kafka. (kafka.server.KafkaServerStartable)
[2019-12-10 13:03:10,382] INFO shutting down (kafka.server.KafkaServer)
[2019-12-10 13:08:40,160] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:08:40,191] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:08:40,191] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:08:40,196] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:08:40,196] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-12-10 13:08:40,218] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:08:40,218] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-12-10 13:08:40,229] INFO Server environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,229] INFO Server environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,230] INFO Server environment:java.version=1.8.0_191 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,230] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,231] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,232] INFO Server environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,238] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,242] INFO Server environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,243] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,244] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,246] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,246] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,247] INFO Server environment:user.name=avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,248] INFO Server environment:user.home=C:\Users\avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,249] INFO Server environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,260] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,260] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,261] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:08:40,283] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-12-10 13:08:40,287] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:09:01,288] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:09:01,847] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:09:01,849] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:09:01,880] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:09:01,890] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,890] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,890] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,891] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,891] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,892] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,901] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,907] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,908] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,909] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,909] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,910] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,911] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,912] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,912] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,916] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:09:01,948] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:09:01,952] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:09:01,954] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:09:01,954] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:49765 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:09:01,965] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:49765 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:09:01,969] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-12-10 13:09:02,133] INFO Established session 0x1000002e3a50000 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:49765 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:09:02,136] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000002e3a50000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:09:02,143] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:09:02,312] INFO Got user-level KeeperException when processing sessionid:0x1000002e3a50000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:09:02,425] INFO Got user-level KeeperException when processing sessionid:0x1000002e3a50000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:09:02,499] INFO Got user-level KeeperException when processing sessionid:0x1000002e3a50000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:09:02,992] INFO Got user-level KeeperException when processing sessionid:0x1000002e3a50000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:09:03,121] INFO Cluster ID = ZViuuwmIQZeN4E08Sdmg_A (kafka.server.KafkaServer)
[2019-12-10 13:09:03,125] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:09:03,187] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:09:03,208] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:09:03,250] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:09:03,251] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:09:03,253] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:09:03,291] INFO Loading logs. (kafka.log.LogManager)
[2019-12-10 13:09:03,380] INFO [Log partition=allValues-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,401] INFO [Log partition=allValues-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 86 ms (kafka.log.Log)
[2019-12-10 13:09:03,413] INFO [Log partition=avgCalculator-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,414] INFO [Log partition=avgCalculator-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:09:03,420] INFO [Log partition=avgCalculator1-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,423] INFO [Log partition=avgCalculator1-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,444] INFO [Log partition=bigSensorValues-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,445] INFO [Log partition=bigSensorValues-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-12-10 13:09:03,453] INFO [Log partition=output-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,454] INFO [Log partition=output-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,463] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,464] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,472] INFO [Log partition=sensorData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,474] INFO [Log partition=sensorData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,483] INFO [Log partition=sensorData1-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,485] INFO [Log partition=sensorData1-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,493] INFO [Log partition=sensorDataDb-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,495] INFO [Log partition=sensorDataDb-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,510] INFO [Log partition=sensorDataDb1-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,512] INFO [Log partition=sensorDataDb1-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-12-10 13:09:03,520] INFO [Log partition=sensorReducedData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,522] INFO [Log partition=sensorReducedData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,529] INFO [Log partition=sensors1-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,531] INFO [Log partition=sensors1-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,544] INFO [Log partition=sensors10-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,546] INFO [Log partition=sensors10-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-12-10 13:09:03,553] INFO [Log partition=sensors10-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,555] INFO [Log partition=sensors10-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,563] INFO [Log partition=sensors10-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,564] INFO [Log partition=sensors10-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,577] INFO [Log partition=sensors2-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,578] INFO [Log partition=sensors2-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-12-10 13:09:03,585] INFO [Log partition=sensors3-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,587] INFO [Log partition=sensors3-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,594] INFO [Log partition=sensors4-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,596] INFO [Log partition=sensors4-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,609] INFO [Log partition=sensorsData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,611] INFO [Log partition=sensorsData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-12-10 13:09:03,617] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,619] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:09:03,627] INFO [Log partition=sensorTransformedData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,629] INFO [Log partition=sensorTransformedData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,635] INFO [Log partition=smallSensorValues-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,639] INFO [Log partition=smallSensorValues-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:09:03,644] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,646] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,652] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,654] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,667] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,669] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-12-10 13:09:03,688] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,690] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-12-10 13:09:03,697] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,698] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:09:03,712] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,713] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-12-10 13:09:03,720] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,723] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:09:03,730] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,731] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:09:03,737] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,739] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,757] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,759] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-12-10 13:09:03,766] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,767] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,779] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,781] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-12-10 13:09:03,786] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,788] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:03,810] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,811] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 19 ms (kafka.log.Log)
[2019-12-10 13:09:03,838] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,840] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 26 ms (kafka.log.Log)
[2019-12-10 13:09:03,896] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,897] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-12-10 13:09:03,943] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,944] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 45 ms (kafka.log.Log)
[2019-12-10 13:09:03,958] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:03,960] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-12-10 13:09:04,044] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,046] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 84 ms (kafka.log.Log)
[2019-12-10 13:09:04,104] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,105] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 56 ms (kafka.log.Log)
[2019-12-10 13:09:04,141] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,142] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[2019-12-10 13:09:04,159] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,160] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-12-10 13:09:04,203] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,206] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 45 ms (kafka.log.Log)
[2019-12-10 13:09:04,240] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,242] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[2019-12-10 13:09:04,321] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,323] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 79 ms (kafka.log.Log)
[2019-12-10 13:09:04,340] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,341] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-12-10 13:09:04,377] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,379] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[2019-12-10 13:09:04,401] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,403] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 22 ms (kafka.log.Log)
[2019-12-10 13:09:04,436] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,438] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 32 ms (kafka.log.Log)
[2019-12-10 13:09:04,461] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,462] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 22 ms (kafka.log.Log)
[2019-12-10 13:09:04,469] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,470] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:04,546] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,548] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 76 ms (kafka.log.Log)
[2019-12-10 13:09:04,553] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,554] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:09:04,559] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,560] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:09:04,577] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,579] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-12-10 13:09:04,584] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,586] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:04,604] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,607] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 19 ms (kafka.log.Log)
[2019-12-10 13:09:04,613] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,615] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:09:04,622] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,623] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:04,630] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,631] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:09:04,638] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,640] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:09:04,647] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,649] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:09:04,662] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,663] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-12-10 13:09:04,670] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,672] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:09:04,677] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,679] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:09:04,684] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,686] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:04,696] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,697] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:09:04,702] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,705] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:09:04,711] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,712] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:04,718] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:04,719] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:09:04,723] INFO Logs loading complete in 1432 ms. (kafka.log.LogManager)
[2019-12-10 13:09:04,735] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-12-10 13:09:04,736] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-12-10 13:09:05,122] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-12-10 13:09:05,180] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-12-10 13:09:05,182] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-12-10 13:09:05,212] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:09:05,215] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:09:05,217] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:09:05,217] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:09:05,230] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:09:05,289] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-12-10 13:09:05,431] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1575976145314,1575976145314,1,0,0,72057606447104000,200,0,24
 (kafka.zk.KafkaZkClient)
[2019-12-10 13:09:05,432] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-2C0H6IK,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2019-12-10 13:09:05,436] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:09:05,664] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:09:05,664] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:09:05,668] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:09:05,690] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:09:05,692] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:09:05,696] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:09:05,794] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2019-12-10 13:09:05,866] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-12-10 13:09:05,899] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:09:05,905] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 13:09:05,905] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:09:05,966] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 13:09:05,996] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-12-10 13:09:06,026] INFO Kafka version: 2.3.1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:09:06,030] INFO Kafka commitId: 18a913733fb71c01 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:09:06,049] INFO Kafka startTimeMs: 1575976146017 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:09:06,086] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-12-10 13:09:06,103] INFO Got user-level KeeperException when processing sessionid:0x1000002e3a50000 type:multi cxid:0x38 zxid:0x1c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:09:43,898] INFO Creating topic sensorDataDB with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:09:43,901] INFO Got user-level KeeperException when processing sessionid:0x1000002e3a50000 type:setData cxid:0x3e zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics/sensorDataDB Error:KeeperErrorCode = NoNode for /config/topics/sensorDataDB (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:09:44,228] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensorDataDB-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:09:44,245] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-12-10 13:09:44,245] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:09:44,285] ERROR Error while creating log for sensorDataDB-0 in dir B:\tmp\kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:09:44,297] INFO [ReplicaManager broker=0] Stopping serving replicas in dir B:\tmp\kafka-logs (kafka.server.ReplicaManager)
[2019-12-10 13:09:44,300] ERROR [ReplicaManager broker=0] Error while making broker the leader for partition Topic: sensorDataDB; Partition: 0; Leader: None; AllReplicas: ; InSyncReplicas:  in dir None (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.KafkaStorageException: Error while creating log for sensorDataDB-0 in dir B:\tmp\kafka-logs
Caused by: java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:09:44,351] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions  and stopped moving logs for partitions  because they are in the failed log directory B:\tmp\kafka-logs. (kafka.server.ReplicaManager)
[2019-12-10 13:09:44,352] INFO Stopping serving logs in dir B:\tmp\kafka-logs (kafka.log.LogManager)
[2019-12-10 13:09:44,358] ERROR Shutdown broker because all log dirs in B:\tmp\kafka-logs have failed (kafka.log.LogManager)
[2019-12-10 13:09:44,918] WARN Exception causing close of session 0x1000002e3a50000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:09:44,920] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:49765 which had sessionid 0x1000002e3a50000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:09:51,955] INFO Expiring session 0x1000002e3a50000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:09:51,958] INFO Processed session termination for sessionid: 0x1000002e3a50000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:10:56,413] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:10:56,418] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:10:56,419] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:10:56,420] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:10:56,422] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-12-10 13:10:56,459] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:10:56,461] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-12-10 13:10:56,481] INFO Server environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,482] INFO Server environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,484] INFO Server environment:java.version=1.8.0_191 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,485] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,486] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,487] INFO Server environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,498] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,503] INFO Server environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,503] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,504] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,505] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,506] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,507] INFO Server environment:user.name=avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,508] INFO Server environment:user.home=C:\Users\avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,508] INFO Server environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,525] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,525] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,528] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:10:56,559] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-12-10 13:10:56,564] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:11:22,701] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:11:23,179] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:11:23,180] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:11:23,205] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:11:23,212] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,212] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,212] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,213] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,213] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,213] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,218] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,222] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,223] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,223] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,224] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,225] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,225] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,226] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,227] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,229] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:11:23,254] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:11:23,257] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:11:23,260] INFO Accepted socket connection from /127.0.0.1:49822 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:11:23,260] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:11:23,269] INFO Client attempting to establish new session at /127.0.0.1:49822 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:11:23,273] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-12-10 13:11:23,389] INFO Established session 0x1000004f8010000 with negotiated timeout 6000 for client /127.0.0.1:49822 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:11:23,391] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000004f8010000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:11:23,396] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:11:23,511] INFO Got user-level KeeperException when processing sessionid:0x1000004f8010000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:11:23,728] INFO Got user-level KeeperException when processing sessionid:0x1000004f8010000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:11:23,802] INFO Got user-level KeeperException when processing sessionid:0x1000004f8010000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:11:24,259] INFO Got user-level KeeperException when processing sessionid:0x1000004f8010000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:11:24,487] INFO Cluster ID = RWiiq045TcSxdl2vlvcLJA (kafka.server.KafkaServer)
[2019-12-10 13:11:24,491] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:11:24,546] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:11:24,564] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:11:24,602] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:11:24,604] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:11:24,604] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:11:24,638] INFO Loading logs. (kafka.log.LogManager)
[2019-12-10 13:11:24,718] INFO [Log partition=allValues-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,734] INFO [Log partition=allValues-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 75 ms (kafka.log.Log)
[2019-12-10 13:11:24,744] INFO [Log partition=avgCalculator-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,745] INFO [Log partition=avgCalculator-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,755] INFO [Log partition=avgCalculator1-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,757] INFO [Log partition=avgCalculator1-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,763] INFO [Log partition=bigSensorValues-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,765] INFO [Log partition=bigSensorValues-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,773] INFO [Log partition=output-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,774] INFO [Log partition=output-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:24,781] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,783] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:24,790] INFO [Log partition=sensorData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,791] INFO [Log partition=sensorData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,798] INFO [Log partition=sensorData1-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,801] INFO [Log partition=sensorData1-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:11:24,807] INFO [Log partition=sensorDataDb-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,808] INFO [Log partition=sensorDataDb-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:24,816] INFO [Log partition=sensorDataDb1-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,817] INFO [Log partition=sensorDataDb1-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:24,825] INFO [Log partition=sensorReducedData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,827] INFO [Log partition=sensorReducedData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:24,834] INFO [Log partition=sensors1-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,835] INFO [Log partition=sensors1-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,841] INFO [Log partition=sensors10-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,843] INFO [Log partition=sensors10-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,849] INFO [Log partition=sensors10-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,851] INFO [Log partition=sensors10-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,857] INFO [Log partition=sensors10-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,859] INFO [Log partition=sensors10-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,867] INFO [Log partition=sensors2-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,868] INFO [Log partition=sensors2-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,875] INFO [Log partition=sensors3-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,876] INFO [Log partition=sensors3-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:24,884] INFO [Log partition=sensors4-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,885] INFO [Log partition=sensors4-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,890] INFO [Log partition=sensorsData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,892] INFO [Log partition=sensorsData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:24,897] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,899] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,907] INFO [Log partition=sensorTransformedData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,911] INFO [Log partition=sensorTransformedData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:11:24,918] INFO [Log partition=smallSensorValues-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,920] INFO [Log partition=smallSensorValues-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:24,938] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,940] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-12-10 13:11:24,946] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,948] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,955] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,956] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,962] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,964] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,971] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,973] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:24,978] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,981] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:24,988] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:24,990] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:24,998] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,000] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:25,007] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,008] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,014] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,016] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,022] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,023] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,028] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,029] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,037] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,038] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,045] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,046] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,052] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,053] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,058] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,060] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,066] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,067] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,072] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,074] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,080] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,084] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:11:25,090] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,094] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:11:25,102] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,105] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-12-10 13:11:25,111] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,113] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,122] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,126] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-12-10 13:11:25,133] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,135] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:11:25,140] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,142] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,148] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,151] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:11:25,157] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,159] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,164] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,167] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:25,173] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,176] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:11:25,184] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,186] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:11:25,192] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,194] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:25,201] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,203] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:25,208] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,209] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,214] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,216] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,221] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,223] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:25,229] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,232] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:25,237] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,238] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,243] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,245] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,251] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,252] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,257] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,258] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,262] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,264] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,270] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,272] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:11:25,277] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,279] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,285] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,286] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,291] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,292] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,297] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,298] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,304] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,305] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,309] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,311] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:11:25,317] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,318] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,322] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:25,324] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:11:25,327] INFO Logs loading complete in 688 ms. (kafka.log.LogManager)
[2019-12-10 13:11:25,338] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-12-10 13:11:25,339] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-12-10 13:11:25,672] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-12-10 13:11:25,719] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-12-10 13:11:25,721] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-12-10 13:11:25,759] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:11:25,759] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:11:25,761] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:11:25,759] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:11:25,776] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:11:25,830] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-12-10 13:11:25,909] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1575976285855,1575976285855,1,0,0,72057615378612224,200,0,24
 (kafka.zk.KafkaZkClient)
[2019-12-10 13:11:25,911] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-2C0H6IK,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2019-12-10 13:11:25,916] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:11:26,078] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:11:26,106] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:11:26,107] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:11:26,115] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:11:26,117] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:11:26,122] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:11:26,145] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2019-12-10 13:11:26,207] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-12-10 13:11:26,242] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:11:26,244] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 13:11:26,244] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:11:26,292] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 13:11:26,306] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-12-10 13:11:26,316] INFO Kafka version: 2.3.1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:11:26,318] INFO Kafka commitId: 18a913733fb71c01 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:11:26,320] INFO Kafka startTimeMs: 1575976286308 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:11:26,326] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-12-10 13:11:26,388] INFO Got user-level KeeperException when processing sessionid:0x1000004f8010000 type:multi cxid:0x38 zxid:0x1c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:11:41,559] INFO Creating topic sensorDataDB with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:11:41,563] INFO Got user-level KeeperException when processing sessionid:0x1000004f8010000 type:setData cxid:0x3e zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics/sensorDataDB Error:KeeperErrorCode = NoNode for /config/topics/sensorDataDB (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:11:41,966] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensorDataDB-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:11:41,978] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-12-10 13:11:41,979] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:11:42,012] ERROR Error while creating log for sensorDataDB-0 in dir B:\tmp\kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:11:42,022] INFO [ReplicaManager broker=0] Stopping serving replicas in dir B:\tmp\kafka-logs (kafka.server.ReplicaManager)
[2019-12-10 13:11:42,026] ERROR [ReplicaManager broker=0] Error while making broker the leader for partition Topic: sensorDataDB; Partition: 0; Leader: None; AllReplicas: ; InSyncReplicas:  in dir None (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.KafkaStorageException: Error while creating log for sensorDataDB-0 in dir B:\tmp\kafka-logs
Caused by: java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:11:42,043] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions  and stopped moving logs for partitions  because they are in the failed log directory B:\tmp\kafka-logs. (kafka.server.ReplicaManager)
[2019-12-10 13:11:42,044] INFO Stopping serving logs in dir B:\tmp\kafka-logs (kafka.log.LogManager)
[2019-12-10 13:11:42,049] ERROR Shutdown broker because all log dirs in B:\tmp\kafka-logs have failed (kafka.log.LogManager)
[2019-12-10 13:11:42,595] WARN Exception causing close of session 0x1000004f8010000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:11:42,597] INFO Closed socket connection for client /127.0.0.1:49822 which had sessionid 0x1000004f8010000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:11:48,955] INFO Expiring session 0x1000004f8010000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:11:48,956] INFO Processed session termination for sessionid: 0x1000004f8010000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:20:37,457] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:20:37,460] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:20:37,461] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:20:37,461] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:20:37,461] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-12-10 13:20:37,485] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:20:37,487] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-12-10 13:20:37,499] INFO Server environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,499] INFO Server environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,500] INFO Server environment:java.version=1.8.0_191 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,502] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,503] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,504] INFO Server environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,510] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,515] INFO Server environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,515] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,516] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,516] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,517] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,518] INFO Server environment:user.name=avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,519] INFO Server environment:user.home=C:\Users\avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,520] INFO Server environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,531] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,532] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,532] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:37,560] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-12-10 13:20:37,563] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:20:43,322] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:20:43,742] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:20:43,744] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:20:43,768] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:20:43,775] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,775] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,776] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,776] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,776] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,776] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,782] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,783] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,784] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,784] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,785] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,789] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,790] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,791] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,791] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,794] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:20:43,817] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:20:43,821] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:20:43,823] INFO Accepted socket connection from /127.0.0.1:50202 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:20:43,823] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:20:43,833] INFO Client attempting to establish new session at /127.0.0.1:50202 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:43,836] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-12-10 13:20:43,905] INFO Established session 0x100000dd5820000 with negotiated timeout 6000 for client /127.0.0.1:50202 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:20:43,908] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100000dd5820000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:20:43,913] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:20:44,102] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:20:44,211] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:20:44,407] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:20:44,872] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:20:44,996] INFO Cluster ID = 89beKc-tQouZVFzVAhV0Kg (kafka.server.KafkaServer)
[2019-12-10 13:20:45,000] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:20:45,074] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:20:45,098] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:20:45,153] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:20:45,155] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:20:45,157] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:20:45,238] INFO Loading logs. (kafka.log.LogManager)
[2019-12-10 13:20:45,248] INFO Logs loading complete in 10 ms. (kafka.log.LogManager)
[2019-12-10 13:20:45,266] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-12-10 13:20:45,270] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-12-10 13:20:46,009] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-12-10 13:20:46,101] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-12-10 13:20:46,111] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-12-10 13:20:46,175] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:20:46,175] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:20:46,184] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:20:46,184] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:20:46,214] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:20:46,260] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-12-10 13:20:46,340] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1575976846295,1575976846295,1,0,0,72057653454569472,200,0,24
 (kafka.zk.KafkaZkClient)
[2019-12-10 13:20:46,343] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-2C0H6IK,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2019-12-10 13:20:46,347] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:20:46,542] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:20:46,547] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:20:46,555] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:20:46,570] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:20:46,572] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:20:46,574] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:20:46,649] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2019-12-10 13:20:46,723] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-12-10 13:20:46,757] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:20:46,760] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:20:46,760] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 13:20:46,854] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-12-10 13:20:46,884] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 13:20:46,888] INFO Kafka version: 2.3.1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:20:46,889] INFO Kafka commitId: 18a913733fb71c01 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:20:46,896] INFO Kafka startTimeMs: 1575976846856 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:20:46,901] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-12-10 13:20:46,960] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:multi cxid:0x38 zxid:0x1c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:21:01,768] INFO Creating topic sensorDataDB with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:21:01,771] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:setData cxid:0x3e zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics/sensorDataDB Error:KeeperErrorCode = NoNode for /config/topics/sensorDataDB (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:21:02,123] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensorDataDB-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:21:02,185] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:02,192] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 52 ms (kafka.log.Log)
[2019-12-10 13:21:02,197] INFO Created log for partition sensorDataDB-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:02,220] INFO [Partition sensorDataDB-0 broker=0] No checkpointed highwatermark is found for partition sensorDataDB-0 (kafka.cluster.Partition)
[2019-12-10 13:21:02,224] INFO Replica loaded for partition sensorDataDB-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:02,229] INFO [Partition sensorDataDB-0 broker=0] sensorDataDB-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:03,050] INFO Creating topic sensDataTransform with configuration {} and initial partition assignment Map(2 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:21:03,052] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:setData cxid:0x48 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/config/topics/sensDataTransform Error:KeeperErrorCode = NoNode for /config/topics/sensDataTransform (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:21:03,801] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensDataTransform-2, sensDataTransform-0, sensDataTransform-1) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:21:03,831] INFO [Log partition=sensDataTransform-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:03,833] INFO [Log partition=sensDataTransform-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[2019-12-10 13:21:03,835] INFO Created log for partition sensDataTransform-2 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:03,837] INFO [Partition sensDataTransform-2 broker=0] No checkpointed highwatermark is found for partition sensDataTransform-2 (kafka.cluster.Partition)
[2019-12-10 13:21:03,838] INFO Replica loaded for partition sensDataTransform-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:03,839] INFO [Partition sensDataTransform-2 broker=0] sensDataTransform-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:04,289] INFO [Log partition=sensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:04,291] INFO [Log partition=sensDataTransform-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[2019-12-10 13:21:04,294] INFO Created log for partition sensDataTransform-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:04,296] INFO [Partition sensDataTransform-0 broker=0] No checkpointed highwatermark is found for partition sensDataTransform-0 (kafka.cluster.Partition)
[2019-12-10 13:21:04,297] INFO Replica loaded for partition sensDataTransform-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:04,298] INFO [Partition sensDataTransform-0 broker=0] sensDataTransform-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:04,462] INFO [Log partition=sensDataTransform-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:04,464] INFO [Log partition=sensDataTransform-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:21:04,467] INFO Created log for partition sensDataTransform-1 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:04,469] INFO [Partition sensDataTransform-1 broker=0] No checkpointed highwatermark is found for partition sensDataTransform-1 (kafka.cluster.Partition)
[2019-12-10 13:21:04,470] INFO Replica loaded for partition sensDataTransform-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:04,471] INFO [Partition sensDataTransform-1 broker=0] sensDataTransform-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:05,147] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(23 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 49 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:21:05,285] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:setData cxid:0x5b zxid:0x2d txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:21:05,742] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-12-10 13:21:07,981] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:21:07,997] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:08,000] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:21:08,005] INFO Created log for partition __consumer_offsets-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:08,008] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-12-10 13:21:08,008] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:08,011] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:08,234] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:08,236] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:21:08,239] INFO Created log for partition __consumer_offsets-29 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:08,242] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-12-10 13:21:08,242] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:08,243] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:08,377] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:08,378] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:08,380] INFO Created log for partition __consumer_offsets-48 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:08,383] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-12-10 13:21:08,383] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:08,384] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:08,541] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:08,544] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-12-10 13:21:08,546] INFO Created log for partition __consumer_offsets-10 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:08,556] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-12-10 13:21:08,558] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:08,560] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:08,708] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:08,710] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:21:08,712] INFO Created log for partition __consumer_offsets-45 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:08,714] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-12-10 13:21:08,715] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:08,716] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:08,865] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:08,867] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:08,869] INFO Created log for partition __consumer_offsets-26 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:08,871] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-12-10 13:21:08,871] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:08,872] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:09,013] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:09,014] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:21:09,016] INFO Created log for partition __consumer_offsets-7 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:09,018] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-12-10 13:21:09,019] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:09,019] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:09,173] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:09,174] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:09,176] INFO Created log for partition __consumer_offsets-42 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:09,178] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-12-10 13:21:09,178] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:09,179] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:09,333] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:09,335] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:09,338] INFO Created log for partition __consumer_offsets-4 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:09,341] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-12-10 13:21:09,341] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:09,342] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:09,626] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:09,628] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:09,630] INFO Created log for partition __consumer_offsets-23 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:09,633] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-12-10 13:21:09,633] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:09,634] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:09,786] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:09,788] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:09,790] INFO Created log for partition __consumer_offsets-1 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:09,792] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-12-10 13:21:09,792] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:09,793] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:09,934] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:09,935] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:21:09,937] INFO Created log for partition __consumer_offsets-20 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:09,940] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-12-10 13:21:09,940] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:09,941] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:10,083] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:10,084] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:10,085] INFO Created log for partition __consumer_offsets-39 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:10,087] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-12-10 13:21:10,088] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:10,088] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:10,407] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:10,409] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:10,411] INFO Created log for partition __consumer_offsets-17 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:10,412] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-12-10 13:21:10,413] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:10,413] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:10,645] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:10,647] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:10,648] INFO Created log for partition __consumer_offsets-36 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:10,650] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-12-10 13:21:10,650] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:10,651] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:10,929] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:10,931] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:10,933] INFO Created log for partition __consumer_offsets-14 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:10,935] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-12-10 13:21:10,936] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:10,936] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:11,214] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:11,220] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[2019-12-10 13:21:11,224] INFO Created log for partition __consumer_offsets-33 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:11,228] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-12-10 13:21:11,230] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:11,231] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:11,487] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:11,491] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 23 ms (kafka.log.Log)
[2019-12-10 13:21:11,497] INFO Created log for partition __consumer_offsets-49 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:11,500] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-12-10 13:21:11,514] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:11,520] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:11,776] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:11,778] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:21:11,782] INFO Created log for partition __consumer_offsets-11 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:11,787] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-12-10 13:21:11,791] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:11,795] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:12,043] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:12,044] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:12,046] INFO Created log for partition __consumer_offsets-30 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:12,047] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-12-10 13:21:12,048] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:12,049] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:12,412] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:12,497] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 106 ms (kafka.log.Log)
[2019-12-10 13:21:12,506] INFO Created log for partition __consumer_offsets-46 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:12,644] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-12-10 13:21:12,645] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:12,648] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:12,899] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:12,902] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-12-10 13:21:12,904] INFO Created log for partition __consumer_offsets-27 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:12,906] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-12-10 13:21:12,906] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:12,907] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:13,236] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:13,238] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:13,240] INFO Created log for partition __consumer_offsets-8 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:13,242] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-12-10 13:21:13,242] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:13,243] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:13,482] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:13,487] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-12-10 13:21:13,492] INFO Created log for partition __consumer_offsets-24 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:13,501] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-12-10 13:21:13,508] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:13,511] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:13,709] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:13,711] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:13,713] INFO Created log for partition __consumer_offsets-43 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:13,716] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-12-10 13:21:13,717] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:13,718] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:13,954] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:13,956] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:13,957] INFO Created log for partition __consumer_offsets-5 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:13,959] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-12-10 13:21:13,960] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:13,961] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:14,218] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:14,221] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 23 ms (kafka.log.Log)
[2019-12-10 13:21:14,227] INFO Created log for partition __consumer_offsets-21 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:14,231] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-12-10 13:21:14,237] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:14,238] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:14,599] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:14,601] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 99 ms (kafka.log.Log)
[2019-12-10 13:21:14,602] INFO Created log for partition __consumer_offsets-2 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:14,604] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-12-10 13:21:14,605] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:14,605] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:14,754] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:14,755] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:21:14,757] INFO Created log for partition __consumer_offsets-40 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:14,759] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-12-10 13:21:14,759] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:14,760] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:15,076] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:15,078] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:15,080] INFO Created log for partition __consumer_offsets-37 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:15,087] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-12-10 13:21:15,089] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:15,091] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:15,671] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:15,706] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 283 ms (kafka.log.Log)
[2019-12-10 13:21:15,722] INFO Created log for partition __consumer_offsets-18 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:15,771] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-12-10 13:21:15,778] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:15,790] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:16,004] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:16,005] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:21:16,007] INFO Created log for partition __consumer_offsets-34 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:16,010] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-12-10 13:21:16,010] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:16,011] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:16,321] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:16,324] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:21:16,326] INFO Created log for partition __consumer_offsets-15 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:16,332] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-12-10 13:21:16,333] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:16,334] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:16,579] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:16,583] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:21:16,589] INFO Created log for partition __consumer_offsets-12 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:16,591] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-12-10 13:21:16,591] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:16,592] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:16,872] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:16,875] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-12-10 13:21:16,877] INFO Created log for partition __consumer_offsets-31 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:16,881] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-12-10 13:21:16,881] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:16,887] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:17,178] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:17,257] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 85 ms (kafka.log.Log)
[2019-12-10 13:21:17,271] INFO Created log for partition __consumer_offsets-9 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:17,306] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-12-10 13:21:17,327] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:17,332] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:17,692] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:17,694] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 133 ms (kafka.log.Log)
[2019-12-10 13:21:17,697] INFO Created log for partition __consumer_offsets-47 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:17,699] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-12-10 13:21:17,700] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:17,701] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:17,936] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:17,937] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-12-10 13:21:17,939] INFO Created log for partition __consumer_offsets-19 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:17,941] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-12-10 13:21:17,941] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:17,942] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:18,153] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:18,154] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:21:18,156] INFO Created log for partition __consumer_offsets-28 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:18,157] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-12-10 13:21:18,158] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:18,159] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:18,391] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:18,392] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:21:18,394] INFO Created log for partition __consumer_offsets-38 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:18,397] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-12-10 13:21:18,397] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:18,398] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:18,619] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:18,621] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:18,626] INFO Created log for partition __consumer_offsets-35 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:18,632] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-12-10 13:21:18,634] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:18,636] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:18,867] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:18,869] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:18,870] INFO Created log for partition __consumer_offsets-44 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:18,872] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-12-10 13:21:18,872] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:18,873] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:19,183] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:19,188] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-12-10 13:21:19,190] INFO Created log for partition __consumer_offsets-6 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:19,197] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-12-10 13:21:19,203] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:19,206] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:19,601] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:19,603] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:21:19,605] INFO Created log for partition __consumer_offsets-25 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:19,607] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-12-10 13:21:19,607] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:19,608] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:19,866] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:19,867] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-12-10 13:21:19,869] INFO Created log for partition __consumer_offsets-16 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:19,871] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-12-10 13:21:19,872] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:19,873] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:20,119] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:20,121] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:21:20,123] INFO Created log for partition __consumer_offsets-22 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:20,124] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-12-10 13:21:20,125] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:20,126] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:20,419] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:20,420] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-12-10 13:21:20,422] INFO Created log for partition __consumer_offsets-41 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:20,424] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-12-10 13:21:20,424] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:20,425] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:20,681] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:20,683] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:21:20,684] INFO Created log for partition __consumer_offsets-32 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:20,685] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-12-10 13:21:20,686] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:20,687] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:20,927] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:20,928] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:21:20,930] INFO Created log for partition __consumer_offsets-3 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:20,932] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-12-10 13:21:20,932] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:20,933] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:21,173] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:21:21,175] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-12-10 13:21:21,177] INFO Created log for partition __consumer_offsets-13 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:21:21,179] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-12-10 13:21:21,180] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:21:21,180] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:21:21,433] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,434] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,435] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,436] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,438] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,438] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,439] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,440] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,440] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,441] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,442] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,443] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,447] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,448] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,449] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,450] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,450] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 16 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,451] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,452] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,453] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,453] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,454] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,454] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,455] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,458] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,460] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,461] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,461] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,462] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,463] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,464] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,464] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,465] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,466] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,467] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,472] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,472] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,473] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,474] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,475] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,475] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,476] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,477] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,478] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,478] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,480] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,483] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,484] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,484] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,485] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,486] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,487] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,487] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,489] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,490] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,490] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,491] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,495] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,497] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,497] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,498] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,499] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,496] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,500] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,501] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,501] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,502] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,503] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,504] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,507] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,508] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,508] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,509] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,510] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,511] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,511] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,512] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,513] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,514] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,519] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,521] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,525] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,525] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,527] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,528] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,531] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,532] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,533] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,534] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,535] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,536] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,537] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,537] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,539] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,542] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,545] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,546] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:21:21,636] INFO [GroupCoordinator 0]: Preparing to rebalance group sens1 in state PreparingRebalance with old generation 0 (__consumer_offsets-30) (reason: Adding new member consumer-2-5475f1d2-1138-4abe-b0c9-eb0831578d40 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:21:21,647] INFO [GroupCoordinator 0]: Stabilized group sens1 generation 1 (__consumer_offsets-30) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:21:21,659] INFO [GroupCoordinator 0]: Assignment received from leader for group sens1 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:22:09,018] INFO Creating topic sensorsRawData with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:22:09,021] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:setData cxid:0x29c zxid:0x95 txntype:-1 reqpath:n/a Error Path:/config/topics/sensorsRawData Error:KeeperErrorCode = NoNode for /config/topics/sensorsRawData (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:22:09,772] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensorsRawData-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:22:09,799] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:22:09,802] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[2019-12-10 13:22:09,805] INFO Created log for partition sensorsRawData-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:22:09,809] INFO [Partition sensorsRawData-0 broker=0] No checkpointed highwatermark is found for partition sensorsRawData-0 (kafka.cluster.Partition)
[2019-12-10 13:22:09,809] INFO Replica loaded for partition sensorsRawData-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:22:09,810] INFO [Partition sensorsRawData-0 broker=0] sensorsRawData-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:22:25,357] INFO Creating topic SensDataTransform with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:22:25,359] INFO Got user-level KeeperException when processing sessionid:0x100000dd5820000 type:setData cxid:0x2a6 zxid:0x9b txntype:-1 reqpath:n/a Error Path:/config/topics/SensDataTransform Error:KeeperErrorCode = NoNode for /config/topics/SensDataTransform (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:22:26,214] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(SensDataTransform-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:22:26,220] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-12-10 13:22:26,220] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:22:26,253] ERROR Error while creating log for SensDataTransform-0 in dir B:\tmp\kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:22:26,260] INFO [ReplicaManager broker=0] Stopping serving replicas in dir B:\tmp\kafka-logs (kafka.server.ReplicaManager)
[2019-12-10 13:22:26,263] ERROR [ReplicaManager broker=0] Error while making broker the leader for partition Topic: SensDataTransform; Partition: 0; Leader: None; AllReplicas: ; InSyncReplicas:  in dir None (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.KafkaStorageException: Error while creating log for SensDataTransform-0 in dir B:\tmp\kafka-logs
Caused by: java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:22:26,273] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, sensDataTransform-1, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, sensDataTransform-2, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, sensorDataDB-0, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, sensorsRawData-0, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, sensDataTransform-0, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:22:26,278] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, sensDataTransform-1, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, sensDataTransform-2, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, sensorDataDB-0, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, sensorsRawData-0, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, sensDataTransform-0, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2019-12-10 13:22:26,322] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions __consumer_offsets-22,sensDataTransform-1,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,sensDataTransform-2,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,sensorDataDB-0,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,sensorsRawData-0,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,sensDataTransform-0,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 and stopped moving logs for partitions  because they are in the failed log directory B:\tmp\kafka-logs. (kafka.server.ReplicaManager)
[2019-12-10 13:22:26,325] INFO Stopping serving logs in dir B:\tmp\kafka-logs (kafka.log.LogManager)
[2019-12-10 13:22:26,335] ERROR Shutdown broker because all log dirs in B:\tmp\kafka-logs have failed (kafka.log.LogManager)
[2019-12-10 13:22:26,890] WARN Exception causing close of session 0x100000dd5820000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:22:26,911] INFO Closed socket connection for client /127.0.0.1:50202 which had sessionid 0x100000dd5820000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:22:33,955] INFO Expiring session 0x100000dd5820000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:22:33,955] INFO Processed session termination for sessionid: 0x100000dd5820000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:26:40,471] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:26:40,476] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:26:40,476] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:26:40,479] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:26:40,479] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-12-10 13:26:40,505] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:26:40,506] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-12-10 13:26:40,518] INFO Server environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,519] INFO Server environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,520] INFO Server environment:java.version=1.8.0_191 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,520] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,521] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,522] INFO Server environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,526] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,530] INFO Server environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,531] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,532] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,532] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,533] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,534] INFO Server environment:user.name=avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,534] INFO Server environment:user.home=C:\Users\avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,535] INFO Server environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,546] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,547] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,547] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:40,566] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-12-10 13:26:40,570] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:26:48,765] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:26:49,436] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:26:49,438] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:26:49,479] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:26:49,491] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,491] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,492] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,492] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,492] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,493] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,503] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,509] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,510] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,511] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,512] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,513] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,514] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,515] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,517] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,523] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:26:49,560] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:26:49,567] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:26:49,571] INFO Accepted socket connection from /127.0.0.1:50637 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:26:49,573] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:26:49,585] INFO Client attempting to establish new session at /127.0.0.1:50637 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:49,589] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-12-10 13:26:49,850] INFO Established session 0x10000135f810000 with negotiated timeout 6000 for client /127.0.0.1:50637 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:26:49,852] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000135f810000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:26:49,856] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:26:50,013] INFO Got user-level KeeperException when processing sessionid:0x10000135f810000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:26:50,116] INFO Got user-level KeeperException when processing sessionid:0x10000135f810000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:26:50,189] INFO Got user-level KeeperException when processing sessionid:0x10000135f810000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:26:50,795] INFO Got user-level KeeperException when processing sessionid:0x10000135f810000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:26:50,995] INFO Cluster ID = MWyHbDz3Tj6HgW-5ytjZkg (kafka.server.KafkaServer)
[2019-12-10 13:26:51,001] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:26:51,069] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:26:51,087] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:26:51,122] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:26:51,123] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:26:51,126] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:26:51,159] INFO Loading logs. (kafka.log.LogManager)
[2019-12-10 13:26:51,238] INFO [Log partition=sensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,256] INFO [Log partition=sensDataTransform-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 75 ms (kafka.log.Log)
[2019-12-10 13:26:51,267] INFO [Log partition=sensDataTransform-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,269] INFO [Log partition=sensDataTransform-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,276] INFO [Log partition=sensDataTransform-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,278] INFO [Log partition=sensDataTransform-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,286] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,287] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,295] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,296] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,304] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,306] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,313] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,314] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,322] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,324] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,331] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,332] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,340] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,342] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,350] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,352] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,358] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,360] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,368] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,370] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,377] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,378] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,386] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,387] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,395] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,397] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,405] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,407] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,413] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,414] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,421] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,422] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,428] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,429] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,437] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,438] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,444] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,446] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,452] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,454] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,459] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,461] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,469] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,471] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,477] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,479] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,485] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,488] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,493] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,494] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,504] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,506] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,513] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,514] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,522] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,524] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,531] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,535] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,540] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,542] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,547] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,549] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,555] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,558] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,564] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,568] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,573] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,575] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,580] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,583] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,588] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,589] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,595] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,596] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,604] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,606] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,613] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,616] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,623] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,625] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,633] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,636] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,643] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,646] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,653] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,655] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,660] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,662] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,668] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,670] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,675] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,677] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-12-10 13:26:51,683] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,685] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:26:51,690] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,692] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,697] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,701] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:26:51,706] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,708] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,713] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,715] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,721] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:51,723] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:26:51,726] INFO Logs loading complete in 567 ms. (kafka.log.LogManager)
[2019-12-10 13:26:51,738] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-12-10 13:26:51,739] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-12-10 13:26:52,103] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-12-10 13:26:52,149] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-12-10 13:26:52,153] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-12-10 13:26:52,240] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:26:52,302] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:26:52,246] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:26:52,246] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:26:52,306] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:26:52,368] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-12-10 13:26:52,481] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1575977212394,1575977212394,1,0,0,72057677244596224,200,0,24
 (kafka.zk.KafkaZkClient)
[2019-12-10 13:26:52,482] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-2C0H6IK,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2019-12-10 13:26:52,485] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:26:52,737] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:26:52,737] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:26:52,737] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:26:52,754] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2019-12-10 13:26:52,762] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:26:52,765] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:26:52,772] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 8 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:26:52,838] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-12-10 13:26:52,889] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:26:52,893] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:26:52,900] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 13:26:52,976] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 13:26:52,993] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-12-10 13:26:53,039] INFO Kafka version: 2.3.1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:26:53,041] INFO Kafka commitId: 18a913733fb71c01 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:26:53,058] INFO Kafka startTimeMs: 1575977213000 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:26:53,070] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-12-10 13:26:53,095] INFO Got user-level KeeperException when processing sessionid:0x10000135f810000 type:multi cxid:0x38 zxid:0x1c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:26:53,261] INFO Creating topic SensDataTransform with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:26:53,261] INFO Creating topic sensDataTransform with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:26:53,266] INFO Got user-level KeeperException when processing sessionid:0x10000135f810000 type:setData cxid:0x43 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics/SensDataTransform Error:KeeperErrorCode = NoNode for /config/topics/SensDataTransform (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:26:53,267] INFO Got user-level KeeperException when processing sessionid:0x10000135f810000 type:setData cxid:0x44 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/sensDataTransform Error:KeeperErrorCode = NoNode for /config/topics/sensDataTransform (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:26:53,408] INFO [KafkaApi-0] Auto creation of topic SensDataTransform with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-12-10 13:26:53,431] INFO [KafkaApi-0] Auto creation of topic sensDataTransform with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-12-10 13:26:53,461] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(23 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 49 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:26:53,468] INFO Got user-level KeeperException when processing sessionid:0x10000135f810000 type:setData cxid:0x50 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:26:53,828] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-12-10 13:26:54,027] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(SensDataTransform-0, sensDataTransform-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:26:54,050] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-12-10 13:26:54,050] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:26:54,082] ERROR Error while creating log for SensDataTransform-0 in dir B:\tmp\kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:26:54,093] INFO [ReplicaManager broker=0] Stopping serving replicas in dir B:\tmp\kafka-logs (kafka.server.ReplicaManager)
[2019-12-10 13:26:54,097] ERROR [ReplicaManager broker=0] Error while making broker the leader for partition Topic: SensDataTransform; Partition: 0; Leader: None; AllReplicas: ; InSyncReplicas:  in dir None (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.KafkaStorageException: Error while creating log for SensDataTransform-0 in dir B:\tmp\kafka-logs
Caused by: java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:26:54,257] INFO [Partition sensDataTransform-0 broker=0] No checkpointed highwatermark is found for partition sensDataTransform-0 (kafka.cluster.Partition)
[2019-12-10 13:26:54,265] INFO Replica loaded for partition sensDataTransform-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:26:54,271] INFO [Partition sensDataTransform-0 broker=0] sensDataTransform-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:26:54,944] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensDataTransform-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:26:54,946] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(sensDataTransform-0) (kafka.server.ReplicaAlterLogDirsManager)
[2019-12-10 13:26:54,957] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions sensDataTransform-0 and stopped moving logs for partitions  because they are in the failed log directory B:\tmp\kafka-logs. (kafka.server.ReplicaManager)
[2019-12-10 13:26:54,958] INFO Stopping serving logs in dir B:\tmp\kafka-logs (kafka.log.LogManager)
[2019-12-10 13:26:54,966] ERROR Shutdown broker because all log dirs in B:\tmp\kafka-logs have failed (kafka.log.LogManager)
[2019-12-10 13:26:55,513] WARN Exception causing close of session 0x10000135f810000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:26:55,514] INFO Closed socket connection for client /127.0.0.1:50637 which had sessionid 0x10000135f810000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:27:00,955] INFO Expiring session 0x10000135f810000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:00,955] INFO Processed session termination for sessionid: 0x10000135f810000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:27:36,212] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:27:36,218] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:27:36,219] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:27:36,220] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:27:36,221] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-12-10 13:27:36,247] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:27:36,248] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-12-10 13:27:36,262] INFO Server environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,262] INFO Server environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,263] INFO Server environment:java.version=1.8.0_191 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,265] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,265] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,267] INFO Server environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,273] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,274] INFO Server environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,275] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,275] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,276] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,280] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,280] INFO Server environment:user.name=avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,281] INFO Server environment:user.home=C:\Users\avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,282] INFO Server environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,294] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,294] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,295] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:27:36,317] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-12-10 13:27:36,320] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:29:02,538] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:29:03,076] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:29:03,078] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:29:03,109] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:29:03,118] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,119] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,119] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,120] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,120] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,120] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,126] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,128] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,129] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,133] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,134] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,135] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,135] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,136] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,137] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,140] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:29:03,171] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:29:03,176] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:29:03,178] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:29:03,178] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:50849 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:29:03,189] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:50849 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:29:03,191] INFO Creating new log file: log.46 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-12-10 13:29:03,328] INFO Established session 0x100001439720000 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:50849 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:29:03,331] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x100001439720000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:29:03,339] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:29:03,428] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0x1 zxid:0x47 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,471] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0x2 zxid:0x48 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,504] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0x3 zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,528] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0x4 zxid:0x4a txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,554] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0x5 zxid:0x4b txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,589] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0x6 zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,614] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0x7 zxid:0x4d txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,638] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0x8 zxid:0x4e txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,663] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0x9 zxid:0x4f txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,688] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0xa zxid:0x50 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,712] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0xb zxid:0x51 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,738] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0xc zxid:0x52 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:03,773] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:create cxid:0xd zxid:0x53 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:04,077] INFO Cluster ID = MWyHbDz3Tj6HgW-5ytjZkg (kafka.server.KafkaServer)
[2019-12-10 13:29:04,082] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:29:04,145] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:29:04,163] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:29:04,199] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:29:04,203] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:29:04,203] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:29:04,244] INFO Loading logs. (kafka.log.LogManager)
[2019-12-10 13:29:04,253] INFO Logs loading complete in 9 ms. (kafka.log.LogManager)
[2019-12-10 13:29:04,267] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-12-10 13:29:04,271] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-12-10 13:29:04,693] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-12-10 13:29:04,749] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-12-10 13:29:04,753] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-12-10 13:29:04,784] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:29:04,788] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:29:04,789] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:29:04,790] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:29:04,803] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:29:04,866] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-12-10 13:29:04,952] INFO Stat of the created znode at /brokers/ids/0 is: 84,84,1575977344888,1575977344888,1,0,0,72057680901046272,200,0,84
 (kafka.zk.KafkaZkClient)
[2019-12-10 13:29:04,952] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-2C0H6IK,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 84 (kafka.zk.KafkaZkClient)
[2019-12-10 13:29:04,955] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:29:05,135] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:29:05,163] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:29:05,163] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:29:05,179] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:29:05,181] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:29:05,183] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:29:05,235] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:1000,blockEndProducerId:1999) by writing to Zk with path version 2 (kafka.coordinator.transaction.ProducerIdManager)
[2019-12-10 13:29:05,321] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:29:05,326] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:29:05,379] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 13:29:05,478] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-12-10 13:29:05,517] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 13:29:05,528] INFO Kafka version: 2.3.1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:29:05,625] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(SensDataTransform-0, sensDataTransform-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:29:05,556] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x69 zxid:0x57 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,628] INFO Kafka commitId: 18a913733fb71c01 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:29:05,652] INFO Kafka startTimeMs: 1575977345523 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:29:05,656] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-12-10 13:29:05,740] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x6f zxid:0x58 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/22 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,779] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x70 zxid:0x59 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/30 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,815] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x71 zxid:0x5a txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/8 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,842] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:29:05,846] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x72 zxid:0x5b txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/21 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,859] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x73 zxid:0x5c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/4 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,865] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 100 ms (kafka.log.Log)
[2019-12-10 13:29:05,878] INFO Created log for partition SensDataTransform-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:29:05,868] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x74 zxid:0x5d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/27 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,890] INFO [Partition SensDataTransform-0 broker=0] No checkpointed highwatermark is found for partition SensDataTransform-0 (kafka.cluster.Partition)
[2019-12-10 13:29:05,890] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x75 zxid:0x5e txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/7 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,898] INFO Replica loaded for partition SensDataTransform-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:29:05,899] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x76 zxid:0x5f txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/9 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,908] INFO [Partition SensDataTransform-0 broker=0] SensDataTransform-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:29:05,909] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x77 zxid:0x60 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/46 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,922] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x78 zxid:0x61 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/25 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,925] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x79 zxid:0x62 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/35 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,928] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x7c zxid:0x63 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/41 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,929] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x7d zxid:0x64 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/33 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,943] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x7e zxid:0x65 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/23 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,946] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x7f zxid:0x66 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/49 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:05,948] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x80 zxid:0x67 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/47 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:06,172] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x81 zxid:0x68 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/16 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:06,186] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x82 zxid:0x69 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/28 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:06,190] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x83 zxid:0x6a txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/31 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:06,191] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x84 zxid:0x6b txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/36 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:06,192] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x85 zxid:0x6c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/42 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:06,193] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x86 zxid:0x6d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/3 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:06,195] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x87 zxid:0x6e txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/18 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:06,221] INFO Got user-level KeeperException when processing sessionid:0x100001439720000 type:multi cxid:0x88 zxid:0x6f txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets/partitions/37 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:29:06,643] INFO [Log partition=sensDataTransform-0, dir=B:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-12-10 13:29:06,644] INFO [Log partition=sensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:29:07,022] ERROR Error while creating log for sensDataTransform-0 in dir B:\tmp\kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:29:07,035] INFO [ReplicaManager broker=0] Stopping serving replicas in dir B:\tmp\kafka-logs (kafka.server.ReplicaManager)
[2019-12-10 13:29:07,133] ERROR [ReplicaManager broker=0] Error while making broker the leader for partition Topic: sensDataTransform; Partition: 0; Leader: None; AllReplicas: ; InSyncReplicas:  in dir None (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.KafkaStorageException: Error while creating log for sensDataTransform-0 in dir B:\tmp\kafka-logs
Caused by: java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:29:07,351] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(SensDataTransform-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:29:07,352] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(SensDataTransform-0) (kafka.server.ReplicaAlterLogDirsManager)
[2019-12-10 13:29:07,360] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions SensDataTransform-0 and stopped moving logs for partitions  because they are in the failed log directory B:\tmp\kafka-logs. (kafka.server.ReplicaManager)
[2019-12-10 13:29:07,361] INFO Stopping serving logs in dir B:\tmp\kafka-logs (kafka.log.LogManager)
[2019-12-10 13:29:07,367] ERROR Shutdown broker because all log dirs in B:\tmp\kafka-logs have failed (kafka.log.LogManager)
[2019-12-10 13:29:07,895] WARN Exception causing close of session 0x100001439720000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:29:07,896] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:50849 which had sessionid 0x100001439720000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:32:42,999] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:32:43,003] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:32:43,004] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:32:43,004] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:32:43,004] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-12-10 13:32:43,028] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:32:43,029] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-12-10 13:32:43,040] INFO Server environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,040] INFO Server environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,041] INFO Server environment:java.version=1.8.0_191 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,041] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,042] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,043] INFO Server environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,047] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,052] INFO Server environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,053] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,054] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,054] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,055] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,056] INFO Server environment:user.name=avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,056] INFO Server environment:user.home=C:\Users\avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,057] INFO Server environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,068] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,069] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,070] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:43,092] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-12-10 13:32:43,096] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:32:47,479] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:32:47,907] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:32:47,908] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:32:47,933] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:32:47,939] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,940] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,940] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,940] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,941] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,941] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,946] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,950] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,951] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,951] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,952] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,953] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,953] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,954] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,954] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,957] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:47,983] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:32:47,986] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:32:47,988] INFO Accepted socket connection from /127.0.0.1:50909 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:32:47,988] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:32:47,995] INFO Client attempting to establish new session at /127.0.0.1:50909 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:47,996] INFO Creating new log file: log.9c (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-12-10 13:32:48,161] INFO Established session 0x1000018e7d80000 with negotiated timeout 6000 for client /127.0.0.1:50909 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:48,164] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000018e7d80000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:32:48,169] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:32:48,244] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0x1 zxid:0x9d txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:48,393] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0x2 zxid:0x9e txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:48,531] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0x3 zxid:0x9f txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:48,666] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0x4 zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:48,802] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0x5 zxid:0xa1 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:48,938] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0x6 zxid:0xa2 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:49,073] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0x7 zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:49,208] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0x8 zxid:0xa4 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:49,345] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0x9 zxid:0xa5 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:49,480] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0xa zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:49,616] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0xb zxid:0xa7 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:49,752] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0xc zxid:0xa8 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:49,887] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:create cxid:0xd zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:50,232] INFO Cluster ID = MWyHbDz3Tj6HgW-5ytjZkg (kafka.server.KafkaServer)
[2019-12-10 13:32:50,236] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:32:50,309] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:32:50,334] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:32:50,383] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:50,385] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:50,387] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:50,438] INFO Loading logs. (kafka.log.LogManager)
[2019-12-10 13:32:50,450] INFO Logs loading complete in 11 ms. (kafka.log.LogManager)
[2019-12-10 13:32:50,470] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-12-10 13:32:50,474] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-12-10 13:32:50,942] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-12-10 13:32:51,000] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-12-10 13:32:51,003] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-12-10 13:32:51,030] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,032] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,034] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,035] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,047] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:32:51,117] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-12-10 13:32:51,136] INFO Got user-level KeeperException when processing sessionid:0x1000018e7d80000 type:multi cxid:0x17 zxid:0xaa txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/brokers/ids/0 Error:KeeperErrorCode = NodeExists for /brokers/ids/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:51,444] ERROR Error while creating ephemeral at /brokers/ids/0, node already exists and owner '72057680901046272' does not match current session '72057701006835712' (kafka.zk.KafkaZkClient$CheckedEphemeral)
[2019-12-10 13:32:51,453] ERROR [KafkaServer id=0] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:122)
	at kafka.zk.KafkaZkClient$CheckedEphemeral.getAfterNodeExists(KafkaZkClient.scala:1784)
	at kafka.zk.KafkaZkClient$CheckedEphemeral.create(KafkaZkClient.scala:1722)
	at kafka.zk.KafkaZkClient.checkedEphemeralCreate(KafkaZkClient.scala:1689)
	at kafka.zk.KafkaZkClient.registerBroker(KafkaZkClient.scala:97)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:262)
	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)
	at kafka.Kafka$.main(Kafka.scala:84)
	at kafka.Kafka.main(Kafka.scala)
[2019-12-10 13:32:51,457] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-12-10 13:32:51,477] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2019-12-10 13:32:51,484] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2019-12-10 13:32:51,502] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2019-12-10 13:32:51,504] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:32:51,506] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:32:51,506] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:32:51,524] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:32:51,527] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:32:51,529] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2019-12-10 13:32:51,531] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2019-12-10 13:32:51,532] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,635] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,635] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,637] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,833] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,833] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,834] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,837] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,837] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,838] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:51,955] INFO Expiring session 0x100001439720000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:32:51,955] INFO Processed session termination for sessionid: 0x100001439720000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:52,039] INFO [ExpirationReaper-0-ElectPreferredLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:52,039] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:32:52,045] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2019-12-10 13:32:52,130] INFO Shutting down. (kafka.log.LogManager)
[2019-12-10 13:32:52,160] INFO Shutdown complete. (kafka.log.LogManager)
[2019-12-10 13:32:52,162] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:32:52,164] INFO Processed session termination for sessionid: 0x1000018e7d80000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:32:52,343] INFO Session: 0x1000018e7d80000 closed (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:32:52,346] INFO EventThread shut down for session: 0x1000018e7d80000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:32:52,343] WARN Unable to read additional data from client sessionid 0x1000018e7d80000, likely client has closed socket (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:32:52,347] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:32:52,351] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:52,349] INFO Closed socket connection for client /127.0.0.1:50909 which had sessionid 0x1000018e7d80000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:32:52,386] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:52,386] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:52,388] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:53,390] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:53,390] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:53,390] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:54,392] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:54,392] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:32:54,395] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2019-12-10 13:32:54,437] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2019-12-10 13:32:54,446] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
[2019-12-10 13:32:54,448] ERROR Exiting Kafka. (kafka.server.KafkaServerStartable)
[2019-12-10 13:32:54,452] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-12-10 13:33:25,829] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:33:25,836] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:33:25,837] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:33:25,837] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:33:25,838] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-12-10 13:33:25,873] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:33:25,874] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-12-10 13:33:25,894] INFO Server environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,895] INFO Server environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,896] INFO Server environment:java.version=1.8.0_191 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,896] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,897] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,898] INFO Server environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,906] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,911] INFO Server environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,912] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,912] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,913] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,914] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,915] INFO Server environment:user.name=avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,916] INFO Server environment:user.home=C:\Users\avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,917] INFO Server environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,936] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,937] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,938] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:25,970] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-12-10 13:33:25,975] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:33:33,488] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:33:33,933] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:33:33,935] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:33:33,959] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:33:33,966] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,967] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,967] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,967] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,967] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,968] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,972] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,977] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,977] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,978] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,979] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,979] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,980] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,981] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,981] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:33,984] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:33:34,009] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:33:34,013] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:33:34,015] INFO Accepted socket connection from /127.0.0.1:50933 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:33:34,015] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:33:34,025] INFO Client attempting to establish new session at /127.0.0.1:50933 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:34,029] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-12-10 13:33:34,249] INFO Established session 0x10000198f270000 with negotiated timeout 6000 for client /127.0.0.1:50933 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:33:34,252] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000198f270000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:33:34,257] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:33:34,516] INFO Got user-level KeeperException when processing sessionid:0x10000198f270000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:33:35,066] INFO Got user-level KeeperException when processing sessionid:0x10000198f270000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:33:35,474] INFO Got user-level KeeperException when processing sessionid:0x10000198f270000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:33:37,423] INFO Got user-level KeeperException when processing sessionid:0x10000198f270000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:33:38,220] INFO Cluster ID = qZv9dq03QIqENnRX8ffkSw (kafka.server.KafkaServer)
[2019-12-10 13:33:38,225] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:33:38,294] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:33:38,312] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:33:38,350] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:33:38,353] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:33:38,355] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:33:38,385] INFO Log directory B:\tmp\kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-12-10 13:33:38,395] INFO Loading logs. (kafka.log.LogManager)
[2019-12-10 13:33:38,404] INFO Logs loading complete in 8 ms. (kafka.log.LogManager)
[2019-12-10 13:33:38,422] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-12-10 13:33:38,425] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-12-10 13:33:38,911] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-12-10 13:33:38,977] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-12-10 13:33:38,980] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-12-10 13:33:39,015] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:33:39,018] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:33:39,018] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:33:39,021] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:33:39,035] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:33:39,077] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-12-10 13:33:39,275] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1575977619099,1575977619099,1,0,0,72057703813808128,200,0,24
 (kafka.zk.KafkaZkClient)
[2019-12-10 13:33:39,277] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-2C0H6IK,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2019-12-10 13:33:39,281] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:33:41,054] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:33:41,059] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:33:41,059] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:33:41,079] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:33:41,081] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:33:41,084] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:33:41,264] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2019-12-10 13:33:41,569] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-12-10 13:33:41,609] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:33:41,614] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:33:41,615] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 13:33:41,689] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 13:33:41,705] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-12-10 13:33:41,711] INFO Kafka version: 2.3.1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:33:41,713] INFO Kafka commitId: 18a913733fb71c01 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:33:41,714] INFO Kafka startTimeMs: 1575977621706 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:33:41,734] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-12-10 13:33:41,780] INFO Got user-level KeeperException when processing sessionid:0x10000198f270000 type:multi cxid:0x38 zxid:0x1c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:33:57,629] INFO Creating topic sensorDataDB with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:33:57,634] INFO Got user-level KeeperException when processing sessionid:0x10000198f270000 type:setData cxid:0x3e zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics/sensorDataDB Error:KeeperErrorCode = NoNode for /config/topics/sensorDataDB (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:33:58,892] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensorDataDB-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:33:58,950] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:33:58,958] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[2019-12-10 13:33:58,962] INFO Created log for partition sensorDataDB-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:33:58,964] INFO [Partition sensorDataDB-0 broker=0] No checkpointed highwatermark is found for partition sensorDataDB-0 (kafka.cluster.Partition)
[2019-12-10 13:33:58,967] INFO Replica loaded for partition sensorDataDB-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:33:58,970] INFO [Partition sensorDataDB-0 broker=0] sensorDataDB-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:33:59,677] INFO Creating topic sensDataTransform with configuration {} and initial partition assignment Map(2 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:33:59,679] INFO Got user-level KeeperException when processing sessionid:0x10000198f270000 type:setData cxid:0x48 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/config/topics/sensDataTransform Error:KeeperErrorCode = NoNode for /config/topics/sensDataTransform (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:34:01,012] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensDataTransform-2, sensDataTransform-0, sensDataTransform-1) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:34:01,019] INFO [Log partition=sensDataTransform-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:01,021] INFO [Log partition=sensDataTransform-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:34:01,023] INFO Created log for partition sensDataTransform-2 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:01,025] INFO [Partition sensDataTransform-2 broker=0] No checkpointed highwatermark is found for partition sensDataTransform-2 (kafka.cluster.Partition)
[2019-12-10 13:34:01,027] INFO Replica loaded for partition sensDataTransform-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:01,028] INFO [Partition sensDataTransform-2 broker=0] sensDataTransform-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:01,339] INFO [Log partition=sensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:01,341] INFO [Log partition=sensDataTransform-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:34:01,343] INFO Created log for partition sensDataTransform-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:01,345] INFO [Partition sensDataTransform-0 broker=0] No checkpointed highwatermark is found for partition sensDataTransform-0 (kafka.cluster.Partition)
[2019-12-10 13:34:01,346] INFO Replica loaded for partition sensDataTransform-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:01,347] INFO [Partition sensDataTransform-0 broker=0] sensDataTransform-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:01,652] INFO [Log partition=sensDataTransform-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:01,654] INFO [Log partition=sensDataTransform-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:34:01,656] INFO Created log for partition sensDataTransform-1 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:01,659] INFO [Partition sensDataTransform-1 broker=0] No checkpointed highwatermark is found for partition sensDataTransform-1 (kafka.cluster.Partition)
[2019-12-10 13:34:01,659] INFO Replica loaded for partition sensDataTransform-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:01,660] INFO [Partition sensDataTransform-1 broker=0] sensDataTransform-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:02,348] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(23 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 49 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:34:02,373] INFO Got user-level KeeperException when processing sessionid:0x10000198f270000 type:setData cxid:0x5b zxid:0x2d txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:34:02,912] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-12-10 13:34:06,111] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:34:06,195] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:06,217] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 25 ms (kafka.log.Log)
[2019-12-10 13:34:06,236] INFO Created log for partition __consumer_offsets-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:06,242] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-12-10 13:34:06,246] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:06,248] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:06,546] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:06,548] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 21 ms (kafka.log.Log)
[2019-12-10 13:34:06,554] INFO Created log for partition __consumer_offsets-29 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:06,558] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-12-10 13:34:06,560] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:06,563] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:06,853] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:06,855] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:34:06,858] INFO Created log for partition __consumer_offsets-48 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:06,863] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-12-10 13:34:06,863] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:06,864] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:07,159] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:07,161] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:34:07,164] INFO Created log for partition __consumer_offsets-10 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:07,167] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-12-10 13:34:07,168] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:07,169] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:07,486] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:07,487] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-12-10 13:34:07,489] INFO Created log for partition __consumer_offsets-45 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:07,491] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-12-10 13:34:07,492] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:07,493] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:07,826] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:07,828] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:07,829] INFO Created log for partition __consumer_offsets-26 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:07,831] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-12-10 13:34:07,831] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:07,832] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:08,110] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:08,116] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-12-10 13:34:08,124] INFO Created log for partition __consumer_offsets-7 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:08,129] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-12-10 13:34:08,131] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:08,138] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:08,588] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:08,590] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 138 ms (kafka.log.Log)
[2019-12-10 13:34:08,592] INFO Created log for partition __consumer_offsets-42 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:08,595] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-12-10 13:34:08,595] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:08,596] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:08,860] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:08,861] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-12-10 13:34:08,863] INFO Created log for partition __consumer_offsets-4 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:08,865] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-12-10 13:34:08,866] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:08,867] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:09,164] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:09,166] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-12-10 13:34:09,168] INFO Created log for partition __consumer_offsets-23 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:09,170] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-12-10 13:34:09,171] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:09,172] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:09,563] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:09,564] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 84 ms (kafka.log.Log)
[2019-12-10 13:34:09,567] INFO Created log for partition __consumer_offsets-1 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:09,570] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-12-10 13:34:09,570] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:09,571] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:09,838] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:09,839] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:09,841] INFO Created log for partition __consumer_offsets-20 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:09,843] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-12-10 13:34:09,843] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:09,844] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:10,211] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:10,212] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:10,214] INFO Created log for partition __consumer_offsets-39 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:10,216] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-12-10 13:34:10,216] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:10,217] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:10,481] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:10,483] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:10,485] INFO Created log for partition __consumer_offsets-17 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:10,487] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-12-10 13:34:10,487] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:10,488] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:10,789] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:10,790] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-12-10 13:34:10,792] INFO Created log for partition __consumer_offsets-36 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:10,794] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-12-10 13:34:10,794] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:10,795] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:11,082] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:11,083] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:34:11,084] INFO Created log for partition __consumer_offsets-14 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:11,087] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-12-10 13:34:11,087] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:11,088] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:11,363] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:11,366] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:34:11,368] INFO Created log for partition __consumer_offsets-33 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:11,370] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-12-10 13:34:11,371] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:11,372] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:11,651] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:11,654] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-12-10 13:34:11,657] INFO Created log for partition __consumer_offsets-49 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:11,660] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-12-10 13:34:11,662] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:11,663] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:11,965] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:11,967] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:11,969] INFO Created log for partition __consumer_offsets-11 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:11,971] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-12-10 13:34:11,971] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:11,972] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:12,269] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:12,273] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-12-10 13:34:12,276] INFO Created log for partition __consumer_offsets-30 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:12,280] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-12-10 13:34:12,281] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:12,285] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:12,583] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:12,584] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:12,586] INFO Created log for partition __consumer_offsets-46 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:12,588] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-12-10 13:34:12,589] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:12,590] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:12,974] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:12,975] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-12-10 13:34:12,977] INFO Created log for partition __consumer_offsets-27 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:12,981] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-12-10 13:34:12,981] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:12,983] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:13,264] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:13,266] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:13,267] INFO Created log for partition __consumer_offsets-8 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:13,269] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-12-10 13:34:13,269] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:13,270] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:13,566] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:13,568] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:13,569] INFO Created log for partition __consumer_offsets-24 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:13,571] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-12-10 13:34:13,571] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:13,572] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:13,879] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:13,881] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-12-10 13:34:13,882] INFO Created log for partition __consumer_offsets-43 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:13,884] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-12-10 13:34:13,885] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:13,886] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:14,250] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:14,253] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:34:14,256] INFO Created log for partition __consumer_offsets-5 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:14,259] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-12-10 13:34:14,259] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:14,261] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:14,719] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:14,724] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 209 ms (kafka.log.Log)
[2019-12-10 13:34:14,727] INFO Created log for partition __consumer_offsets-21 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:14,732] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-12-10 13:34:14,733] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:14,735] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:15,009] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:15,011] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:34:15,013] INFO Created log for partition __consumer_offsets-2 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:15,016] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-12-10 13:34:15,017] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:15,018] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:15,290] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:15,292] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[2019-12-10 13:34:15,295] INFO Created log for partition __consumer_offsets-40 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:15,298] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-12-10 13:34:15,300] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:15,300] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:15,987] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:15,995] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 304 ms (kafka.log.Log)
[2019-12-10 13:34:15,998] INFO Created log for partition __consumer_offsets-37 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:16,008] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-12-10 13:34:16,013] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:16,028] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:16,342] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:16,345] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-12-10 13:34:16,349] INFO Created log for partition __consumer_offsets-18 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:16,379] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-12-10 13:34:16,380] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:16,426] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:16,739] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:16,749] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 34 ms (kafka.log.Log)
[2019-12-10 13:34:16,761] INFO Created log for partition __consumer_offsets-34 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:16,774] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-12-10 13:34:16,774] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:16,776] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:17,301] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:17,304] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 25 ms (kafka.log.Log)
[2019-12-10 13:34:17,305] INFO Created log for partition __consumer_offsets-15 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:17,315] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-12-10 13:34:17,330] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:17,341] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:17,751] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:17,774] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 40 ms (kafka.log.Log)
[2019-12-10 13:34:17,776] INFO Created log for partition __consumer_offsets-12 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:17,783] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-12-10 13:34:17,789] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:17,791] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:18,123] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:18,145] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[2019-12-10 13:34:18,149] INFO Created log for partition __consumer_offsets-31 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:18,151] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-12-10 13:34:18,154] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:18,156] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:18,426] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:18,429] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:34:18,432] INFO Created log for partition __consumer_offsets-9 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:18,435] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-12-10 13:34:18,436] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:18,437] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:18,727] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:18,730] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-12-10 13:34:18,734] INFO Created log for partition __consumer_offsets-47 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:18,736] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-12-10 13:34:18,737] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:18,738] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:19,233] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:19,237] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 223 ms (kafka.log.Log)
[2019-12-10 13:34:19,239] INFO Created log for partition __consumer_offsets-19 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:19,243] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-12-10 13:34:19,244] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:19,245] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:19,748] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:19,753] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 177 ms (kafka.log.Log)
[2019-12-10 13:34:19,758] INFO Created log for partition __consumer_offsets-28 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:19,770] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-12-10 13:34:19,816] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:19,837] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:20,207] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:20,218] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 28 ms (kafka.log.Log)
[2019-12-10 13:34:20,221] INFO Created log for partition __consumer_offsets-38 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:20,225] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-12-10 13:34:20,226] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:20,228] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:20,597] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:20,720] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 128 ms (kafka.log.Log)
[2019-12-10 13:34:20,733] INFO Created log for partition __consumer_offsets-35 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:20,764] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-12-10 13:34:20,785] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:20,799] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:21,170] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:21,174] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 20 ms (kafka.log.Log)
[2019-12-10 13:34:21,176] INFO Created log for partition __consumer_offsets-44 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:21,188] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-12-10 13:34:21,189] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:21,190] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:21,349] INFO Creating topic SensDataTransform with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:34:21,351] INFO Got user-level KeeperException when processing sessionid:0x10000198f270000 type:setData cxid:0x2c4 zxid:0x95 txntype:-1 reqpath:n/a Error Path:/config/topics/SensDataTransform Error:KeeperErrorCode = NoNode for /config/topics/SensDataTransform (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:34:22,493] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:22,495] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 457 ms (kafka.log.Log)
[2019-12-10 13:34:22,496] INFO Created log for partition __consumer_offsets-6 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:22,498] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-12-10 13:34:22,499] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:22,500] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:23,075] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:23,076] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-12-10 13:34:23,077] INFO Created log for partition __consumer_offsets-25 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:23,079] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-12-10 13:34:23,080] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:23,081] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:23,434] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:23,438] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[2019-12-10 13:34:23,440] INFO Created log for partition __consumer_offsets-16 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:23,442] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-12-10 13:34:23,443] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:23,447] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:23,720] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:23,722] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:34:23,724] INFO Created log for partition __consumer_offsets-22 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:23,727] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-12-10 13:34:23,727] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:23,729] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:24,072] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:24,073] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:34:24,074] INFO Created log for partition __consumer_offsets-41 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:24,076] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-12-10 13:34:24,078] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:24,079] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:24,263] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:24,265] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:24,266] INFO Created log for partition __consumer_offsets-32 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:24,268] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-12-10 13:34:24,269] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:24,269] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:24,529] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:24,530] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 88 ms (kafka.log.Log)
[2019-12-10 13:34:24,532] INFO Created log for partition __consumer_offsets-3 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:24,536] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-12-10 13:34:24,536] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:24,537] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:24,706] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:24,707] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:34:24,709] INFO Created log for partition __consumer_offsets-13 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:34:24,711] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-12-10 13:34:24,711] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:34:24,712] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:34:24,877] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,880] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,882] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,883] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,884] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,885] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,886] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,887] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 8 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,887] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,891] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,892] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,893] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,894] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,888] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,895] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,896] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,896] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,897] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,899] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,899] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,904] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,904] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,905] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,906] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,907] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,908] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,909] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,909] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,910] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,911] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,914] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,915] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,917] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,917] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,918] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,919] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,920] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,921] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,922] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,922] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,926] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,926] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,927] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,928] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,929] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,929] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,930] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,931] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,932] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,932] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,934] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,933] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,937] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,938] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,939] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,940] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,941] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,941] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,942] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,943] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,944] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,944] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,945] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,946] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,949] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,950] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,951] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,952] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,952] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,953] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,954] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,955] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,955] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,956] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,957] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,958] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,958] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,964] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,964] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,966] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,969] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,970] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,973] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,973] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(SensDataTransform-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:34:24,973] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,975] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,976] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,977] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,978] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,979] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,980] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,981] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,982] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-12-10 13:34:24,985] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:34:24,984] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,987] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,988] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:24,989] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:34:25,026] ERROR Error while creating log for SensDataTransform-0 in dir B:\tmp\kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:34:25,035] INFO [ReplicaManager broker=0] Stopping serving replicas in dir B:\tmp\kafka-logs (kafka.server.ReplicaManager)
[2019-12-10 13:34:25,038] ERROR [ReplicaManager broker=0] Error while making broker the leader for partition Topic: SensDataTransform; Partition: 0; Leader: None; AllReplicas: ; InSyncReplicas:  in dir None (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.KafkaStorageException: Error while creating log for SensDataTransform-0 in dir B:\tmp\kafka-logs
Caused by: java.io.IOException: The requested operation cannot be performed on a file with a user-mapped section open
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:188)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:174)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:174)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:240)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:239)
	at kafka.log.LogSegment.recover(LogSegment.scala:397)
	at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:493)
	at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:608)
	at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log$$anonfun$2.apply(Log.scala:568)
	at kafka.log.Log.retryOnOffsetOverflow(Log.scala:2096)
	at kafka.log.Log.loadSegments(Log.scala:567)
	at kafka.log.Log.<init>(Log.scala:285)
	at kafka.log.Log$.apply(Log.scala:2229)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:706)
	at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:674)
	at scala.Option.getOrElse(Option.scala:121)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:220)
	at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:216)
	at kafka.utils.Pool$$anon$2.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:215)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6$$anonfun$8.apply(Partition.scala:394)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:394)
	at kafka.cluster.Partition$$anonfun$6.apply(Partition.scala:388)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.makeLeader(Partition.scala:388)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1205)
	at kafka.server.ReplicaManager$$anonfun$makeLeaders$4.apply(ReplicaManager.scala:1203)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1203)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1115)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:201)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:117)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:748)
[2019-12-10 13:34:25,041] INFO [GroupCoordinator 0]: Preparing to rebalance group sens3 in state PreparingRebalance with old generation 0 (__consumer_offsets-32) (reason: Adding new member consumer-2-5a1524c6-9b61-4147-b616-3992891972fd with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:34:25,047] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, sensDataTransform-1, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, sensDataTransform-2, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, sensorDataDB-0, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, sensDataTransform-0, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:34:25,053] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, sensDataTransform-1, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, sensDataTransform-2, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, sensorDataDB-0, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, sensDataTransform-0, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2019-12-10 13:34:25,072] INFO [GroupCoordinator 0]: Stabilized group sens3 generation 1 (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:34:25,103] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions __consumer_offsets-22,sensDataTransform-1,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,sensDataTransform-2,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,sensorDataDB-0,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,sensDataTransform-0,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 and stopped moving logs for partitions  because they are in the failed log directory B:\tmp\kafka-logs. (kafka.server.ReplicaManager)
[2019-12-10 13:34:25,133] INFO Stopping serving logs in dir B:\tmp\kafka-logs (kafka.log.LogManager)
[2019-12-10 13:34:25,138] INFO [GroupCoordinator 0]: Assignment received from leader for group sens3 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:34:25,146] ERROR Shutdown broker because all log dirs in B:\tmp\kafka-logs have failed (kafka.log.LogManager)
[2019-12-10 13:34:25,702] WARN Exception causing close of session 0x10000198f270000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:34:25,704] INFO Closed socket connection for client /127.0.0.1:50933 which had sessionid 0x10000198f270000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 13:34:33,954] INFO Expiring session 0x10000198f270000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:34:33,955] INFO Processed session termination for sessionid: 0x10000198f270000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:40:24,410] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:40:24,414] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:40:24,415] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:40:24,415] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-12-10 13:40:24,415] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-12-10 13:40:24,439] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-12-10 13:40:24,439] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-12-10 13:40:24,452] INFO Server environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,452] INFO Server environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,453] INFO Server environment:java.version=1.8.0_191 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,454] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,455] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,456] INFO Server environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,461] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,465] INFO Server environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,465] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,466] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,467] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,467] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,468] INFO Server environment:user.name=avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,468] INFO Server environment:user.home=C:\Users\avgry (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,469] INFO Server environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,481] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,481] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,482] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:24,504] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-12-10 13:40:24,508] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:40:33,331] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-12-10 13:40:33,849] INFO starting (kafka.server.KafkaServer)
[2019-12-10 13:40:33,851] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-12-10 13:40:33,881] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:40:33,891] INFO Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,892] INFO Client environment:host.name=DESKTOP-2C0H6IK (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,892] INFO Client environment:java.version=1.8.0_191 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,892] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,893] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_191\jre (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,893] INFO Client environment:java.class.path=B:\Java2\kafka_2.11-2.2.1\libs\activation-1.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\aopalliance-repackaged-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\argparse4j-0.7.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\audience-annotations-0.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\commons-lang3-3.8.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-api-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-basic-auth-extension-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-file-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-json-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-runtime-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\connect-transforms-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\guava-20.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-api-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-locator-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\hk2-utils-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-core-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-databind-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-dataformat-csv-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-datatype-jdk8-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-base-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-jaxrs-json-provider-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-jaxb-annotations-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-paranamer-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jackson-module-scala_2.11-2.10.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.activation-api-1.2.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.annotation-api-1.3.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.inject-2.5.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.ws.rs-api-2.1.5.jar;B:\Java2\kafka_2.11-2.2.1\libs\jakarta.xml.bind-api-2.3.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javassist-3.22.0-CR2.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.servlet-api-3.1.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\javax.ws.rs-api-2.1.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\jaxb-api-2.3.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-client-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-common-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-container-servlet-core-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-hk2-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-media-jaxb-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jersey-server-2.28.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-client-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-continuation-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-http-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-io-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-security-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-server-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlet-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-servlets-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jetty-util-9.4.18.v20190429.jar;B:\Java2\kafka_2.11-2.2.1\libs\jopt-simple-5.0.4.jar;B:\Java2\kafka_2.11-2.2.1\libs\jsr305-3.0.2.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-clients-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-log4j-appender-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-examples-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-scala_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-streams-test-utils-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka-tools-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-javadoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-scaladoc.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test-sources.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1-test.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\kafka_2.11-2.3.1.jar.asc;B:\Java2\kafka_2.11-2.2.1\libs\log4j-1.2.17.jar;B:\Java2\kafka_2.11-2.2.1\libs\lz4-java-1.6.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\maven-artifact-3.6.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\metrics-core-2.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\osgi-resource-locator-1.0.1.jar;B:\Java2\kafka_2.11-2.2.1\libs\paranamer-2.8.jar;B:\Java2\kafka_2.11-2.2.1\libs\plexus-utils-3.2.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\reflections-0.9.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\rocksdbjni-5.18.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-library-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-logging_2.11-3.9.0.jar;B:\Java2\kafka_2.11-2.2.1\libs\scala-reflect-2.11.12.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-api-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\slf4j-log4j12-1.7.26.jar;B:\Java2\kafka_2.11-2.2.1\libs\snappy-java-1.1.7.3.jar;B:\Java2\kafka_2.11-2.2.1\libs\spotbugs-annotations-3.1.9.jar;B:\Java2\kafka_2.11-2.2.1\libs\validation-api-2.0.1.Final.jar;B:\Java2\kafka_2.11-2.2.1\libs\zkclient-0.11.jar;B:\Java2\kafka_2.11-2.2.1\libs\zookeeper-3.4.14.jar;B:\Java2\kafka_2.11-2.2.1\libs\zstd-jni-1.4.0-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,899] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_191\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Git\cmd;B:\Program Files\nodejs\;C:\Users\avgry\AppData\Local\Microsoft\WindowsApps;C:\Users\avgry\AppData\Local\GitHubDesktop\bin;C:\Program Files\heroku\bin;C:\Program Files\apache-maven\bin;C:\Users\avgry\AppData\Roaming\npm;. (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,900] INFO Client environment:java.io.tmpdir=C:\Users\avgry\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,901] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,901] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,906] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,907] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,908] INFO Client environment:user.name=avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,908] INFO Client environment:user.home=C:\Users\avgry (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,909] INFO Client environment:user.dir=B:\Java2\kafka_2.11-2.2.1 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,912] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2019-12-10 13:40:33,941] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:40:33,946] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:40:33,948] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:40:33,948] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:51029 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-12-10 13:40:33,959] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:51029 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:33,963] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-12-10 13:40:34,151] INFO Established session 0x100001ff2030000 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:51029 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-12-10 13:40:34,155] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x100001ff2030000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 13:40:34,159] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 13:40:34,440] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:40:35,143] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:40:35,720] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:40:37,634] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:40:38,409] INFO Cluster ID = fSehsRr9Qqmd4T7iqD2CLA (kafka.server.KafkaServer)
[2019-12-10 13:40:38,412] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:40:38,465] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:40:38,481] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.3-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.3-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-12-10 13:40:38,519] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:40:38,521] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:40:38,524] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 13:40:38,552] INFO Log directory B:\tmp\kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-12-10 13:40:38,564] INFO Loading logs. (kafka.log.LogManager)
[2019-12-10 13:40:38,573] INFO Logs loading complete in 9 ms. (kafka.log.LogManager)
[2019-12-10 13:40:38,590] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-12-10 13:40:38,592] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-12-10 13:40:39,031] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-12-10 13:40:39,081] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-12-10 13:40:39,085] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-12-10 13:40:39,118] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:40:39,120] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:40:39,120] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:40:39,124] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:40:39,138] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 13:40:39,183] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-12-10 13:40:39,375] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1575978039205,1575978039205,1,0,0,72057731242196992,200,0,24
 (kafka.zk.KafkaZkClient)
[2019-12-10 13:40:39,377] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-2C0H6IK,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2019-12-10 13:40:39,382] WARN No meta.properties file under dir B:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-12-10 13:40:39,640] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:40:39,640] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:40:39,660] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 13:40:39,836] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2019-12-10 13:40:39,845] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:40:39,847] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:40:39,853] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:40:40,131] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-12-10 13:40:40,174] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:40:40,178] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 13:40:40,178] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 13:40:40,249] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 13:40:40,263] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-12-10 13:40:40,270] INFO Kafka version: 2.3.1 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:40:40,270] INFO Kafka commitId: 18a913733fb71c01 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:40:40,271] INFO Kafka startTimeMs: 1575978040265 (org.apache.kafka.common.utils.AppInfoParser)
[2019-12-10 13:40:40,285] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-12-10 13:40:40,376] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:multi cxid:0x38 zxid:0x1c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:40:52,782] INFO Creating topic sensorDataDB with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:40:52,786] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:setData cxid:0x3e zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics/sensorDataDB Error:KeeperErrorCode = NoNode for /config/topics/sensorDataDB (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:40:53,981] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensorDataDB-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:40:54,096] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:40:54,106] INFO [Log partition=sensorDataDB-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 98 ms (kafka.log.Log)
[2019-12-10 13:40:54,111] INFO Created log for partition sensorDataDB-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:40:54,114] INFO [Partition sensorDataDB-0 broker=0] No checkpointed highwatermark is found for partition sensorDataDB-0 (kafka.cluster.Partition)
[2019-12-10 13:40:54,119] INFO Replica loaded for partition sensorDataDB-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:40:54,123] INFO [Partition sensorDataDB-0 broker=0] sensorDataDB-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:40:54,845] INFO Creating topic SensDataTransform with configuration {} and initial partition assignment Map(2 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:40:54,847] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:setData cxid:0x48 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/config/topics/SensDataTransform Error:KeeperErrorCode = NoNode for /config/topics/SensDataTransform (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:40:56,227] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(SensDataTransform-0, SensDataTransform-1, SensDataTransform-2) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:40:56,269] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:40:56,271] INFO [Log partition=SensDataTransform-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 38 ms (kafka.log.Log)
[2019-12-10 13:40:56,273] INFO Created log for partition SensDataTransform-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:40:56,276] INFO [Partition SensDataTransform-0 broker=0] No checkpointed highwatermark is found for partition SensDataTransform-0 (kafka.cluster.Partition)
[2019-12-10 13:40:56,276] INFO Replica loaded for partition SensDataTransform-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:40:56,277] INFO [Partition SensDataTransform-0 broker=0] SensDataTransform-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:40:56,470] INFO [Log partition=SensDataTransform-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:40:56,472] INFO [Log partition=SensDataTransform-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:40:56,477] INFO Created log for partition SensDataTransform-1 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:40:56,481] INFO [Partition SensDataTransform-1 broker=0] No checkpointed highwatermark is found for partition SensDataTransform-1 (kafka.cluster.Partition)
[2019-12-10 13:40:56,483] INFO Replica loaded for partition SensDataTransform-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:40:56,485] INFO [Partition SensDataTransform-1 broker=0] SensDataTransform-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:40:56,831] INFO [Log partition=SensDataTransform-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:40:56,833] INFO [Log partition=SensDataTransform-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:40:56,836] INFO Created log for partition SensDataTransform-2 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:40:56,838] INFO [Partition SensDataTransform-2 broker=0] No checkpointed highwatermark is found for partition SensDataTransform-2 (kafka.cluster.Partition)
[2019-12-10 13:40:56,839] INFO Replica loaded for partition SensDataTransform-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:40:56,840] INFO [Partition SensDataTransform-2 broker=0] SensDataTransform-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:40:57,631] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(23 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 49 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:40:57,756] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:setData cxid:0x5b zxid:0x2d txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:40:58,555] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-12-10 13:41:01,319] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:41:01,333] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:01,335] INFO [Log partition=__consumer_offsets-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:01,337] INFO Created log for partition __consumer_offsets-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:01,339] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-12-10 13:41:01,340] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:01,341] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:01,551] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:01,553] INFO [Log partition=__consumer_offsets-29, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 19 ms (kafka.log.Log)
[2019-12-10 13:41:01,554] INFO Created log for partition __consumer_offsets-29 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:01,557] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-12-10 13:41:01,557] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:01,558] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:01,734] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:01,736] INFO [Log partition=__consumer_offsets-48, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:01,738] INFO Created log for partition __consumer_offsets-48 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:01,740] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-12-10 13:41:01,741] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:01,741] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:01,941] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:01,943] INFO [Log partition=__consumer_offsets-10, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:01,946] INFO Created log for partition __consumer_offsets-10 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:01,948] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-12-10 13:41:01,949] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:01,950] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:02,159] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:02,160] INFO [Log partition=__consumer_offsets-45, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-12-10 13:41:02,163] INFO Created log for partition __consumer_offsets-45 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:02,165] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-12-10 13:41:02,165] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:02,166] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:02,492] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:02,494] INFO [Log partition=__consumer_offsets-26, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:02,495] INFO Created log for partition __consumer_offsets-26 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:02,497] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-12-10 13:41:02,497] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:02,498] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:02,684] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:02,686] INFO [Log partition=__consumer_offsets-7, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:02,687] INFO Created log for partition __consumer_offsets-7 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:02,690] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-12-10 13:41:02,691] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:02,692] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:02,870] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:02,871] INFO [Log partition=__consumer_offsets-42, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:41:02,873] INFO Created log for partition __consumer_offsets-42 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:02,875] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-12-10 13:41:02,876] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:02,877] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:03,049] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:03,051] INFO [Log partition=__consumer_offsets-4, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:03,053] INFO Created log for partition __consumer_offsets-4 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:03,055] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-12-10 13:41:03,056] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:03,056] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:03,247] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:03,250] INFO [Log partition=__consumer_offsets-23, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 22 ms (kafka.log.Log)
[2019-12-10 13:41:03,253] INFO Created log for partition __consumer_offsets-23 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:03,259] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-12-10 13:41:03,260] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:03,262] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:03,526] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:03,528] INFO [Log partition=__consumer_offsets-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:03,529] INFO Created log for partition __consumer_offsets-1 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:03,531] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-12-10 13:41:03,532] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:03,533] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:03,729] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:03,731] INFO [Log partition=__consumer_offsets-20, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-12-10 13:41:03,737] INFO Created log for partition __consumer_offsets-20 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:03,739] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-12-10 13:41:03,739] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:03,742] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:03,939] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:03,942] INFO [Log partition=__consumer_offsets-39, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 20 ms (kafka.log.Log)
[2019-12-10 13:41:03,944] INFO Created log for partition __consumer_offsets-39 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:03,947] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-12-10 13:41:03,948] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:03,954] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:04,152] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:04,154] INFO [Log partition=__consumer_offsets-17, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:04,158] INFO Created log for partition __consumer_offsets-17 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:04,160] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-12-10 13:41:04,160] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:04,161] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:04,619] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:04,620] INFO [Log partition=__consumer_offsets-36, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 276 ms (kafka.log.Log)
[2019-12-10 13:41:04,622] INFO Created log for partition __consumer_offsets-36 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:04,757] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-12-10 13:41:04,757] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:04,759] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:05,068] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:05,071] INFO [Log partition=__consumer_offsets-14, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-12-10 13:41:05,073] INFO Created log for partition __consumer_offsets-14 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:05,076] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-12-10 13:41:05,076] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:05,078] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:05,246] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:05,249] INFO [Log partition=__consumer_offsets-33, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:05,253] INFO Created log for partition __consumer_offsets-33 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:05,270] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-12-10 13:41:05,272] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:05,274] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:05,465] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:05,468] INFO [Log partition=__consumer_offsets-49, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 21 ms (kafka.log.Log)
[2019-12-10 13:41:05,470] INFO Created log for partition __consumer_offsets-49 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:05,474] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-12-10 13:41:05,475] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:05,476] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:05,651] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:05,653] INFO [Log partition=__consumer_offsets-11, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:05,657] INFO Created log for partition __consumer_offsets-11 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:05,660] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-12-10 13:41:05,660] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:05,661] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:05,841] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:05,842] INFO [Log partition=__consumer_offsets-30, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-12-10 13:41:05,843] INFO Created log for partition __consumer_offsets-30 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:05,846] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-12-10 13:41:05,846] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:05,847] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:06,022] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:06,023] INFO [Log partition=__consumer_offsets-46, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:06,026] INFO Created log for partition __consumer_offsets-46 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:06,029] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-12-10 13:41:06,032] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:06,036] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:06,328] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:06,329] INFO [Log partition=__consumer_offsets-27, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-12-10 13:41:06,332] INFO Created log for partition __consumer_offsets-27 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:06,334] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-12-10 13:41:06,334] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:06,335] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:06,679] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:06,680] INFO [Log partition=__consumer_offsets-8, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:06,682] INFO Created log for partition __consumer_offsets-8 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:06,684] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-12-10 13:41:06,684] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:06,686] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:06,908] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:06,910] INFO [Log partition=__consumer_offsets-24, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:06,911] INFO Created log for partition __consumer_offsets-24 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:06,913] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-12-10 13:41:06,914] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:06,915] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:07,137] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:07,138] INFO [Log partition=__consumer_offsets-43, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:07,140] INFO Created log for partition __consumer_offsets-43 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:07,142] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-12-10 13:41:07,142] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:07,143] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:07,354] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:07,356] INFO [Log partition=__consumer_offsets-5, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:07,359] INFO Created log for partition __consumer_offsets-5 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:07,361] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-12-10 13:41:07,362] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:07,363] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:07,571] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:07,573] INFO [Log partition=__consumer_offsets-21, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:07,575] INFO Created log for partition __consumer_offsets-21 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:07,578] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-12-10 13:41:07,579] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:07,580] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:07,893] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:07,894] INFO [Log partition=__consumer_offsets-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-12-10 13:41:07,897] INFO Created log for partition __consumer_offsets-2 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:07,899] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-12-10 13:41:07,899] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:07,900] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:08,077] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:08,079] INFO [Log partition=__consumer_offsets-40, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:08,080] INFO Created log for partition __consumer_offsets-40 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:08,082] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-12-10 13:41:08,083] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:08,084] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:08,271] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:08,273] INFO [Log partition=__consumer_offsets-37, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:08,274] INFO Created log for partition __consumer_offsets-37 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:08,276] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-12-10 13:41:08,276] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:08,277] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:08,465] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:08,466] INFO [Log partition=__consumer_offsets-18, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:08,468] INFO Created log for partition __consumer_offsets-18 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:08,471] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-12-10 13:41:08,472] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:08,473] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:08,805] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:08,807] INFO [Log partition=__consumer_offsets-34, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 153 ms (kafka.log.Log)
[2019-12-10 13:41:08,809] INFO Created log for partition __consumer_offsets-34 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:08,812] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-12-10 13:41:08,813] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:08,814] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:09,012] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:09,014] INFO [Log partition=__consumer_offsets-15, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:09,015] INFO Created log for partition __consumer_offsets-15 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:09,018] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-12-10 13:41:09,018] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:09,019] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:09,331] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:09,333] INFO [Log partition=__consumer_offsets-12, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:09,335] INFO Created log for partition __consumer_offsets-12 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:09,337] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-12-10 13:41:09,337] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:09,338] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:09,653] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:09,654] INFO [Log partition=__consumer_offsets-31, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 112 ms (kafka.log.Log)
[2019-12-10 13:41:09,656] INFO Created log for partition __consumer_offsets-31 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:09,661] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-12-10 13:41:09,662] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:09,663] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:09,869] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:09,871] INFO [Log partition=__consumer_offsets-9, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:09,872] INFO Created log for partition __consumer_offsets-9 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:09,874] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-12-10 13:41:09,875] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:09,875] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:10,058] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:10,060] INFO [Log partition=__consumer_offsets-47, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:10,061] INFO Created log for partition __consumer_offsets-47 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:10,063] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-12-10 13:41:10,064] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:10,065] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:10,391] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:10,392] INFO [Log partition=__consumer_offsets-19, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:10,393] INFO Created log for partition __consumer_offsets-19 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:10,395] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-12-10 13:41:10,396] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:10,396] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:10,613] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:10,614] INFO [Log partition=__consumer_offsets-28, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:10,616] INFO Created log for partition __consumer_offsets-28 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:10,617] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-12-10 13:41:10,618] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:10,618] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:10,847] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:10,857] INFO [Log partition=__consumer_offsets-38, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 23 ms (kafka.log.Log)
[2019-12-10 13:41:10,869] INFO Created log for partition __consumer_offsets-38 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:10,879] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-12-10 13:41:10,881] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:10,882] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:11,090] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:11,097] INFO [Log partition=__consumer_offsets-35, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-12-10 13:41:11,104] INFO Created log for partition __consumer_offsets-35 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:11,108] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-12-10 13:41:11,109] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:11,110] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:11,309] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:11,312] INFO [Log partition=__consumer_offsets-44, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-12-10 13:41:11,314] INFO Created log for partition __consumer_offsets-44 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:11,317] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-12-10 13:41:11,319] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:11,320] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:11,540] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:11,542] INFO [Log partition=__consumer_offsets-6, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-12-10 13:41:11,544] INFO Created log for partition __consumer_offsets-6 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:11,546] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-12-10 13:41:11,548] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:11,549] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:11,756] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:11,758] INFO [Log partition=__consumer_offsets-25, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:11,760] INFO Created log for partition __consumer_offsets-25 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:11,762] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-12-10 13:41:11,762] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:11,764] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:11,961] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:11,964] INFO [Log partition=__consumer_offsets-16, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-12-10 13:41:11,965] INFO Created log for partition __consumer_offsets-16 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:11,969] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-12-10 13:41:11,969] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:11,970] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:12,165] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:12,166] INFO [Log partition=__consumer_offsets-22, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:12,167] INFO Created log for partition __consumer_offsets-22 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:12,169] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-12-10 13:41:12,170] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:12,170] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:12,370] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:12,372] INFO [Log partition=__consumer_offsets-41, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-12-10 13:41:12,373] INFO Created log for partition __consumer_offsets-41 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:12,375] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-12-10 13:41:12,375] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:12,376] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:12,575] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:12,576] INFO [Log partition=__consumer_offsets-32, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:12,578] INFO Created log for partition __consumer_offsets-32 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:12,579] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-12-10 13:41:12,580] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:12,581] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:12,779] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:12,781] INFO [Log partition=__consumer_offsets-3, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:12,783] INFO Created log for partition __consumer_offsets-3 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:12,784] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-12-10 13:41:12,785] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:12,786] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:12,996] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:12,997] INFO [Log partition=__consumer_offsets-13, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:13,000] INFO Created log for partition __consumer_offsets-13 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:13,002] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-12-10 13:41:13,002] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:13,003] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:13,322] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,326] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,327] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,329] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,330] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,332] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,334] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,335] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 10 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,336] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,338] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,338] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,340] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,341] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,341] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,342] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,343] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,344] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,339] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,347] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,349] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,348] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,349] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,351] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,350] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,352] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,354] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,353] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,354] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,356] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,359] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,361] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,362] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,362] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,363] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,360] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,365] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,366] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,367] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,370] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,365] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,371] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,372] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,373] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,374] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,374] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,375] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,376] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,377] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,378] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,379] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,379] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,383] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,384] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,384] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,385] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,386] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,387] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,388] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,388] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,389] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,389] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,390] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,391] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,394] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,395] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,396] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,396] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,397] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,398] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,399] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,399] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,400] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,401] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,401] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,402] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,403] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,408] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,409] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,410] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,411] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,412] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,412] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,414] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,415] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,420] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,423] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,424] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,429] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,433] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,434] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,435] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,437] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,438] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,439] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,440] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,443] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,444] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,445] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,446] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,447] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 13:41:13,466] INFO [GroupCoordinator 0]: Preparing to rebalance group anonymous.5ff6f54d-2112-4388-80cc-5499c4c39b80 in state PreparingRebalance with old generation 0 (__consumer_offsets-15) (reason: Adding new member consumer-2-ea7a580e-95b9-4ac5-87fd-159b1fa81533 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:41:13,477] INFO [GroupCoordinator 0]: Stabilized group anonymous.5ff6f54d-2112-4388-80cc-5499c4c39b80 generation 1 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:41:13,491] INFO [GroupCoordinator 0]: Assignment received from leader for group anonymous.5ff6f54d-2112-4388-80cc-5499c4c39b80 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:41:33,716] INFO Creating topic sensorsRawData with configuration {} and initial partition assignment Map(2 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:41:33,719] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:setData cxid:0x275 zxid:0x95 txntype:-1 reqpath:n/a Error Path:/config/topics/sensorsRawData Error:KeeperErrorCode = NoNode for /config/topics/sensorsRawData (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:41:34,782] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensorsRawData-1, sensorsRawData-2, sensorsRawData-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:41:34,816] INFO [Log partition=sensorsRawData-1, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:34,817] INFO [Log partition=sensorsRawData-1, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 32 ms (kafka.log.Log)
[2019-12-10 13:41:34,819] INFO Created log for partition sensorsRawData-1 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:34,821] INFO [Partition sensorsRawData-1 broker=0] No checkpointed highwatermark is found for partition sensorsRawData-1 (kafka.cluster.Partition)
[2019-12-10 13:41:34,821] INFO Replica loaded for partition sensorsRawData-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:34,822] INFO [Partition sensorsRawData-1 broker=0] sensorsRawData-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:35,178] INFO [Log partition=sensorsRawData-2, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:35,179] INFO [Log partition=sensorsRawData-2, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-12-10 13:41:35,181] INFO Created log for partition sensorsRawData-2 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:35,183] INFO [Partition sensorsRawData-2 broker=0] No checkpointed highwatermark is found for partition sensorsRawData-2 (kafka.cluster.Partition)
[2019-12-10 13:41:35,184] INFO Replica loaded for partition sensorsRawData-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:35,184] INFO [Partition sensorsRawData-2 broker=0] sensorsRawData-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:35,392] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:41:35,394] INFO [Log partition=sensorsRawData-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 26 ms (kafka.log.Log)
[2019-12-10 13:41:35,396] INFO Created log for partition sensorsRawData-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:41:35,398] INFO [Partition sensorsRawData-0 broker=0] No checkpointed highwatermark is found for partition sensorsRawData-0 (kafka.cluster.Partition)
[2019-12-10 13:41:35,398] INFO Replica loaded for partition sensorsRawData-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:41:35,399] INFO [Partition sensorsRawData-0 broker=0] sensorsRawData-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:41:35,876] INFO [GroupCoordinator 0]: Preparing to rebalance group sens1 in state PreparingRebalance with old generation 0 (__consumer_offsets-30) (reason: Adding new member consumer-2-a5810d46-f3bf-4743-8476-a8fbee29e38f with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:41:35,890] INFO [GroupCoordinator 0]: Stabilized group sens1 generation 1 (__consumer_offsets-30) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:41:35,900] INFO [GroupCoordinator 0]: Assignment received from leader for group sens1 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:45:28,183] INFO Creating topic sensorsDataDB with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-12-10 13:45:28,252] INFO Got user-level KeeperException when processing sessionid:0x100001ff2030000 type:setData cxid:0x285 zxid:0x9f txntype:-1 reqpath:n/a Error Path:/config/topics/sensorsDataDB Error:KeeperErrorCode = NoNode for /config/topics/sensorsDataDB (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 13:45:29,780] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(sensorsDataDB-0) (kafka.server.ReplicaFetcherManager)
[2019-12-10 13:45:29,810] INFO [Log partition=sensorsDataDB-0, dir=B:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-12-10 13:45:29,813] INFO [Log partition=sensorsDataDB-0, dir=B:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 30 ms (kafka.log.Log)
[2019-12-10 13:45:29,815] INFO Created log for partition sensorsDataDB-0 in B:\tmp\kafka-logs with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.3-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2019-12-10 13:45:29,817] INFO [Partition sensorsDataDB-0 broker=0] No checkpointed highwatermark is found for partition sensorsDataDB-0 (kafka.cluster.Partition)
[2019-12-10 13:45:29,818] INFO Replica loaded for partition sensorsDataDB-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-12-10 13:45:29,819] INFO [Partition sensorsDataDB-0 broker=0] sensorsDataDB-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-12-10 13:45:30,390] INFO [GroupCoordinator 0]: Preparing to rebalance group anonymous.7d0eddbe-31f3-43f7-a6f5-534903bca049 in state PreparingRebalance with old generation 0 (__consumer_offsets-22) (reason: Adding new member consumer-2-26cd6e8f-924f-40da-b654-4aff7f0c2ee6 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:45:30,396] INFO [GroupCoordinator 0]: Stabilized group anonymous.7d0eddbe-31f3-43f7-a6f5-534903bca049 generation 1 (__consumer_offsets-22) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:45:30,405] INFO [GroupCoordinator 0]: Assignment received from leader for group anonymous.7d0eddbe-31f3-43f7-a6f5-534903bca049 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 13:50:39,886] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 38 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 14:00:39,847] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 14:10:39,847] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 14:20:39,848] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 14:30:39,848] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 14:40:13,627] INFO [GroupCoordinator 0]: Member consumer-2-ea7a580e-95b9-4ac5-87fd-159b1fa81533 in group anonymous.5ff6f54d-2112-4388-80cc-5499c4c39b80 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 14:40:13,630] INFO [GroupCoordinator 0]: Preparing to rebalance group anonymous.5ff6f54d-2112-4388-80cc-5499c4c39b80 in state PreparingRebalance with old generation 1 (__consumer_offsets-15) (reason: removing member consumer-2-ea7a580e-95b9-4ac5-87fd-159b1fa81533 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 14:40:13,646] INFO [GroupCoordinator 0]: Group anonymous.5ff6f54d-2112-4388-80cc-5499c4c39b80 with generation 2 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 14:40:13,754] INFO [GroupCoordinator 0]: Member consumer-2-a5810d46-f3bf-4743-8476-a8fbee29e38f in group sens1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 14:40:13,754] INFO [GroupCoordinator 0]: Preparing to rebalance group sens1 in state PreparingRebalance with old generation 1 (__consumer_offsets-30) (reason: removing member consumer-2-a5810d46-f3bf-4743-8476-a8fbee29e38f on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 14:40:13,755] INFO [GroupCoordinator 0]: Group sens1 with generation 2 is now empty (__consumer_offsets-30) (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 14:40:39,868] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 20 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 14:50:39,848] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 15:00:39,848] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-12-10 15:10:39,002] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-12-10 15:10:39,042] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2019-12-10 15:10:39,172] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2019-12-10 15:10:39,182] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 15:10:39,184] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 15:10:39,185] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-12-10 15:10:39,196] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2019-12-10 15:10:39,224] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2019-12-10 15:10:39,243] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[2019-12-10 15:10:39,249] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2019-12-10 15:10:39,258] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[2019-12-10 15:10:39,262] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,388] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,388] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,408] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 15:10:39,412] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0 (kafka.coordinator.transaction.ProducerIdManager)
[2019-12-10 15:10:39,414] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2019-12-10 15:10:39,415] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 15:10:39,416] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 15:10:39,416] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-12-10 15:10:39,418] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-12-10 15:10:39,420] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 15:10:39,421] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,568] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,568] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,569] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,588] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,588] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,599] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2019-12-10 15:10:39,602] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2019-12-10 15:10:39,602] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 15:10:39,603] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 15:10:39,603] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-12-10 15:10:39,605] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2019-12-10 15:10:39,628] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2019-12-10 15:10:39,630] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2019-12-10 15:10:39,631] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2019-12-10 15:10:39,632] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,739] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,739] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,741] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,789] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,789] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,791] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,990] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,990] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:39,992] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:40,190] INFO [ExpirationReaper-0-ElectPreferredLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:40,190] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-12-10 15:10:40,240] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2019-12-10 15:10:40,242] INFO Shutting down. (kafka.log.LogManager)
[2019-12-10 15:10:40,291] INFO [ProducerStateManager partition=__consumer_offsets-22] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:40,590] INFO [ProducerStateManager partition=__consumer_offsets-30] Writing producer snapshot at offset 3486 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:40,881] INFO [ProducerStateManager partition=SensDataTransform-1] Writing producer snapshot at offset 1160 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:41,346] INFO [ProducerStateManager partition=sensorsRawData-2] Writing producer snapshot at offset 1160 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:42,340] INFO [ProducerStateManager partition=SensDataTransform-0] Writing producer snapshot at offset 1160 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:43,169] INFO [ProducerStateManager partition=__consumer_offsets-15] Writing producer snapshot at offset 3486 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:43,618] INFO [ProducerStateManager partition=sensorsRawData-1] Writing producer snapshot at offset 1160 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:44,206] INFO [ProducerStateManager partition=sensorDataDB-0] Writing producer snapshot at offset 10 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:44,835] INFO [ProducerStateManager partition=sensorsRawData-0] Writing producer snapshot at offset 1161 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:45,920] INFO [ProducerStateManager partition=SensDataTransform-2] Writing producer snapshot at offset 1161 (kafka.log.ProducerStateManager)
[2019-12-10 15:10:46,160] INFO Shutdown complete. (kafka.log.LogManager)
[2019-12-10 15:10:46,226] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 15:10:46,232] INFO Processed session termination for sessionid: 0x100001ff2030000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-12-10 15:10:46,360] INFO Session: 0x100001ff2030000 closed (org.apache.zookeeper.ZooKeeper)
[2019-12-10 15:10:46,362] INFO EventThread shut down for session: 0x100001ff2030000 (org.apache.zookeeper.ClientCnxn)
[2019-12-10 15:10:46,362] WARN Unable to read additional data from client sessionid 0x100001ff2030000, likely client has closed socket (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 15:10:46,366] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)
[2019-12-10 15:10:46,368] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 15:10:46,374] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:51029 which had sessionid 0x100001ff2030000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-12-10 15:10:47,033] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 15:10:47,033] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 15:10:47,035] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 15:10:47,102] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 15:10:47,102] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 15:10:47,102] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 15:10:48,094] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 15:10:48,094] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-12-10 15:10:48,099] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2019-12-10 15:10:48,336] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2019-12-10 15:10:48,344] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
